from __future__ import annotations

"""Ozon parser (Ñ„Ğ¸ĞºÑ Ğ°Ğ²Ğ³ÑƒÑÑ‚ 2025, web* patch)
Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ + Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸/Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ + Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹.

Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ:
â€¢ ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ±ĞµÑ€Ñ‘Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ· widgetStates["webDescription-2983278-pdpPage2column-2"].
â€¢ Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ±ĞµÑ€Ñ‘Ğ¼ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸Ğ· widgetStates["webCharacteristics-3282540-pdpPage2column-2"].
â€¢ Ğ•ÑĞ»Ğ¸ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ web*-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ (Ñ€ĞµĞ´ĞºĞ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ¶Ğ½ĞµĞ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ²ÑĞµÑ… widgetStates.
â€¢ âŸ¶ NEW: ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· reviewshelfpaginator.
    â–¸ Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ PDP Ğ¿Ğ¾ÑĞ»Ğµ ĞµÑ‘ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ reviewshelfpaginator,
      Ğ³Ğ´Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ reviewsVariantMode=1.
    â–¸ JSON-ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ° Ğ²Ğ¸Ğ´Ğ° "webListReviews-*-reviewshelfpaginator-*" ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Â«ĞºĞ°Ğº ĞµÑÑ‚ÑŒÂ»
      Ğ² ĞºĞ°Ñ‚Ğ°Ğ»Ğ¾Ğ³ ./reviews_raw (Ğ¾Ğ´Ğ¸Ğ½ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ·Ğ¶Ğµ ĞµÑ‘ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ.
    â–¸ ĞœĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ğ°ÑƒĞ·Ñ‹ Ğ¾Ñ‚ 0.9 Ğ´Ğ¾ 1.5 Ñ.
"""

import argparse
import asyncio
import json
import random
import re
import urllib.parse as u
from datetime import date, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from camoufox.async_api import AsyncCamoufox

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEARCH_URL = "https://www.ozon.ru/search/?text=Ğ¿ÑƒÑÑ€&category=9373&page=1"
OUTPUT_GOODS_CSV   = "ozon_puer_full.csv"
OUTPUT_REVIEWS_CSV = "ozon_puer_reviews.csv"  # Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ (ÑÑ‹Ñ€Ñ‹Ğµ JSON-Ñ„Ğ°Ğ¹Ğ»Ñ‹ ĞºĞ»Ğ°Ğ´ÑƒÑ‚ÑÑ Ğ² reviews_raw)
RAW_REVIEWS_DIR    = Path("reviews_raw")
MAX_PAGES    = 3      # 0 = unlimited
CONCURRENCY  = 6      # PDP concurrency
HEADLESS     = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ ĞµĞ³ÑĞºÑĞ¿Ñ‹ / ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENTRY_RE      = re.compile(r"/api/entrypoint-api\.bx/page/json/v2\?url=%2Fsearch%2F")
PRICE_RE      = re.compile(r"\d+")
# REVIEW_WIDGET_RE = re.compile(r"^webListReviews-.*-reviewshelfpaginator-\d+")
REVIEW_WIDGET_RE = re.compile(r"webListReviews-\d+-reviewshelfpaginator-\d+")
NARROW_SPACE  = "\u2009"
TODAY: date   = date.today()
MONTHS_RU     = {
    "ÑĞ½Ğ²Ğ°Ñ€Ñ": 1, "Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ": 2, "Ğ¼Ğ°Ñ€Ñ‚Ğ°": 3, "Ğ°Ğ¿Ñ€ĞµĞ»Ñ": 4,
    "Ğ¼Ğ°Ñ": 5, "Ğ¸ÑĞ½Ñ": 6, "Ğ¸ÑĞ»Ñ": 7, "Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°": 8,
    "ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ": 9, "Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ": 10, "Ğ½Ğ¾ÑĞ±Ñ€Ñ": 11, "Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ": 12,
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="Ozon parser")
parser.add_argument("--reviews", action="store_true", help="Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ (ÑÑ‹Ñ€Ñ‹Ğµ JSON-Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ² ./reviews_raw)")
parser.add_argument("--reviews-limit", type=int, default=0,
                    help="ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€ (0 = Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°)")
args, _ = parser.parse_known_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def digits_only(text: str) -> str:
    return re.sub(r"[^0-9.]", "", text.replace(" ", "")
                 .replace(NARROW_SPACE, "").replace("\xa0", ""))

def clean_price(text: Optional[str]) -> Optional[int]:
    if not text:
        return None
    m = PRICE_RE.search(digits_only(text))
    return int(m.group()) if m else None

def parse_delivery(text: Optional[str]) -> Optional[date]:
    if not text:
        return None
    txt = text.lower().strip()
    if txt == "ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ":
        return TODAY
    if txt == "Ğ·Ğ°Ğ²Ñ‚Ñ€Ğ°":
        return TODAY + timedelta(days=1)
    if txt == "Ğ¿Ğ¾ÑĞ»ĞµĞ·Ğ°Ğ²Ñ‚Ñ€Ğ°":
        return TODAY + timedelta(days=2)
    m = re.match(r"(\d{1,2})\s+([Ğ°-ÑÑ‘]+)", txt)
    if m:
        day, month_name = int(m.group(1)), m.group(2)
        month = MONTHS_RU.get(month_name)
        if month:
            year = TODAY.year
            parsed = date(year, month, day)
            if parsed < TODAY - timedelta(days=2):
                parsed = date(year + 1, month, day)
            return parsed
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¡Ñ‹Ñ€Ñ‹Ğ¹ save-ÑÑ‚Ğ°Ğ± Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_reviews_raw(sku: str, page_no: int, raw_json: str) -> None:
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ "ĞºĞ°Ğº ĞµÑÑ‚ÑŒ" ÑÑ‚Ñ€Ğ¾ĞºÑƒ JSON-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²."""
    RAW_REVIEWS_DIR.mkdir(exist_ok=True)
    out_path = RAW_REVIEWS_DIR / f"{sku}_page{page_no}.json"
    out_path.write_text(raw_json, encoding="utf-8")

    # Ğ’ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ ÑÑĞ´Ğ° Ğ²ĞµÑ€Ğ½Ñ‘Ñ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²
    # ĞŸĞ¾ĞºĞ° Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ / Ğ½Ğµ Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ SEARCH ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_items(json_page: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº item-ÑĞ»Ğ¾Ğ² Ğ¸Ğ· SERP JSON."""
    for raw in json_page.get("widgetStates", {}).values():
        try:
            obj = json.loads(raw)
        except Exception:
            continue
        items = obj.get("items")
        if isinstance(items, list):
            good = [it for it in items if isinstance(it, dict) and "mainState" in it]
            if len(good) >= 5:
                return good
    return []


def grab_item(it: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    sku = str(it.get("sku") or "")
    if not sku:
        return None

    link = it.get("action", {}).get("link", "").split("?")[0]
    name = next((b["textAtom"]["text"] for b in it["mainState"] if b["type"] == "textAtom"), None)

    price_curr = price_old = None
    price_block = next((b for b in it["mainState"] if b["type"] == "priceV2"), None)
    if price_block:
        for p in price_block["priceV2"].get("price", []):
            if p.get("textStyle") == "PRICE":
                price_curr = clean_price(p.get("text"))
            elif p.get("textStyle") == "ORIGINAL_PRICE":
                price_old = clean_price(p.get("text"))

    rating = reviews_cnt = None
    for b in it["mainState"]:
        if b["type"] == "labelList" and "rating" in json.dumps(b):
            labels = b["labelList"].get("items", [])
            if labels:
                rating_txt = digits_only(labels[0]["title"])
                rating = float(rating_txt) if rating_txt else None
                if len(labels) > 1:
                    rev_txt = digits_only(labels[1]["title"])
                    reviews_cnt = int(rev_txt) if rev_txt else None
            break

    img_urls = [i["image"]["link"] for i in it.get("tileImage", {}).get("items", [])[:10]]
    images = "|".join(img_urls)

    mb = it.get("multiButton", {}).get("ozonButton", {}).get("addToCart", {})
    delivery_raw = mb.get("actionButton", {}).get("title") if mb else None
    delivery_date = parse_delivery(delivery_raw)
    max_qty = mb.get("quantityButton", {}).get("maxItems") if mb else None

    return {
        "sku": sku,
        "link": link,
        "name": name,
        "price_curr": price_curr,
        "price_old": price_old,
        "rating": rating,
        "reviews_cnt": reviews_cnt,
        "images": images,
        "delivery_day_raw": delivery_raw,
        "delivery_date": delivery_date,
        "max_qty": max_qty,
        "first_seen": TODAY,
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PDP helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Â«Ğ¡Ñ‹Ñ€Ñ‹ĞµÂ» web-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ ------------------------------------------------------

def collect_raw_widget(json_page: Dict[str, Any], prefix: str) -> str:
    """Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ widgetStates-value, Ñ‡ĞµĞ¹ ĞºĞ»ÑÑ‡ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ prefix."""
    for k, raw in json_page.get("widgetStates", {}).items():
        if k.startswith(prefix):
            return raw            # ÑÑ‚Ğ¾ ÑƒĞ¶Ğµ JSON-ÑÑ‚Ñ€Ğ¾ĞºĞ°
    return ""

# 2.  ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ / Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ----------------------------------------------

def collect_description(json_page: Dict[str, Any]) -> str:
    # webDescription-* Ğ² Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğµ
    raw = collect_raw_widget(json_page, "webDescription-")
    return raw or ""


def collect_characteristics(json_page: Dict[str, Any]) -> str:
    # webCharacteristics-* Ğ² Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğµ
    raw = collect_raw_widget(json_page, "webCharacteristics-")
    return raw or ""

# 3.  Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² ------------------------------------------------

async def fetch_reviews_shelf(
    ctx,
    headers: Dict[str, str],
    path_part: str,
    sku: str,
    limit: int | None = None,
) -> None:
    """
    Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ° reviewshelfpaginator Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Â«ÑÑ‹Ñ€Ñ‹ĞµÂ» JSON-ÑÑ‚Ñ€Ğ¾ĞºĞ¸.
    Ğ•ÑĞ»Ğ¸ limit > 0 â€” Ğ¿Ñ€ĞµĞºÑ€Ğ°Ñ‰Ğ°ĞµĞ¼, ĞºĞ°Ğº Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸ Ğ½Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµ limit ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†.
    """
    # URL Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾-ÑÑ‚Ğ°Ñ€Ğ¾Ğ¼Ñƒ
    next_path: str | None = (
        f"{path_part}?layout_container=reviewshelfpaginator"
        "&layout_page_index=1&reviewsVariantMode=1"
    )

    page_no = 1           # ÑÑ‡Ñ‘Ñ‚Ñ‡Ğ¸Ğº, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ĞºÑ€Ğ°ÑĞ¸Ğ²Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»Ñ‹
    total_saved = 0

    while next_path:
        shelf_api = (
            "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2?url="
            + u.quote(next_path, safe="")
        )

        resp = await ctx.request.get(shelf_api, headers=headers)
        if resp.status != 200:
            break                     # 404/500 â€” Ğ´Ğ°Ğ»ÑŒÑˆĞµ Ğ¸Ğ´Ñ‚Ğ¸ Ğ½ĞµĞ»ÑŒĞ·Ñ

        data = await resp.json()

        # Â«Ğ¡Ñ‹Ñ€Ğ¾Ğ¹Â» JSON ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ°
        raw_widget = next(
            (
                v
                for k, v in data.get("widgetStates", {}).items()
                if REVIEW_WIDGET_RE.match(k)
            ),
            None,
        )
        if not raw_widget:
            break                     # Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ½ĞµÑ‚

        save_reviews_raw(sku, page_no, raw_widget)
        total_saved += 1

        if limit and total_saved >= limit:
            break                    # Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†

        # --- Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ URL ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ---
        next_path = data.get("nextPage") or data.get("pageInfo", {}).get("url")
        page_no += 1

        await asyncio.sleep(random.uniform(0.9, 1.5))

# 4.  ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ PDP Ğ³Ñ€Ğ°Ğ±Ğ±ĞµÑ€ ----------------------------------------------------

async def grab_pdp(ctx, headers: Dict[str, str], row: Dict[str, Any],
                   want_reviews: bool, reviews_limit: int
                   ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:

    path_part = row["link"].split("ozon.ru")[-1]
    pdp_api = (
        "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2?url=" +
        u.quote(f"{path_part}?layout_container=pdpPage2column&layout_page_index=2", safe="")
    )

    resp = await ctx.request.get(pdp_api, headers=headers)
    if resp.status != 200:
        return row, []

    pdp_json = await resp.json()

    # Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ / Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
    row["charcs_json"] = collect_characteristics(pdp_json) or None
    row["description"] = collect_description(pdp_json) or None

    # Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹
    if want_reviews:
        await fetch_reviews_shelf(ctx, headers, path_part, row["sku"], reviews_limit or None)

    await asyncio.sleep(random.uniform(0.8, 1.6))
    return row, []  # reviews Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼/Ğ½Ğµ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ñ€ÑƒÑ‚Ğ¸Ğ½Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def grab() -> None:
    start_page = int(re.search(r"page=(\d+)", SEARCH_URL).group(1))

    async with AsyncCamoufox(headless=HEADLESS) as browser:
        ctx  = await browser.new_context(locale="ru-RU")
        page = await ctx.new_page()

        first_headers: Dict[str, str] = {}
        first_json: Optional[Dict[str, Any]] = None

        async def capture_entry(resp):
            nonlocal first_headers, first_json
            if ENTRY_RE.search(resp.url) and resp.status == 200 and not first_json:
                raw_h = await resp.request.all_headers()
                # ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸/brotli
                first_headers = {k: v for k, v in raw_h.items() if not k.startswith(":")}
                first_headers.pop("accept-encoding", None)
                first_json = await resp.json()

        page.on("response", capture_entry)

        await page.goto(SEARCH_URL, timeout=60_000, wait_until="domcontentloaded")
        await asyncio.sleep(random.uniform(1.5, 3.0))
        await page.mouse.wheel(0, 3000)
        await asyncio.sleep(random.uniform(1.5, 3.0))

        if not first_json:
            print("â›” Entrypoint-API Ğ½Ğµ Ğ¿Ğ¾Ğ¹Ğ¼Ğ°Ğ½ â€” Ğ°Ğ½Ñ‚Ğ¸Ğ±Ğ¾Ñ‚/Ğ½Ğ¾Ğ²Ñ‹Ğ¹ layout.")
            return

        # ----- 1. collect SERP -----
        all_rows, seen = [], set()
        json_page, page_no = first_json, start_page

        while True:
            for it in extract_items(json_page):
                row = grab_item(it)
                if row and row["sku"] not in seen:
                    seen.add(row["sku"])
                    all_rows.append(row)
            print(f"âœ“ page {page_no}: {len(seen)} Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²")

            page_no += 1
            if MAX_PAGES and page_no > start_page + MAX_PAGES - 1:
                break

            next_api = (
                "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2?url=" +
                u.quote(f"/search/?text=Ğ¿ÑƒÑÑ€&category=9373&page={page_no}", safe="")
            )
            resp = await ctx.request.get(next_api, headers=first_headers)
            if resp.status != 200:
                print("â›” HTTP", resp.status, "Ğ½Ğ°", page_no)
                break
            json_page = await resp.json()
            await asyncio.sleep(random.uniform(1.0, 1.8))

        # ----- 2. PDP -----
        sem = asyncio.Semaphore(CONCURRENCY)
        results: List[Tuple[Dict[str, Any], List[Dict[str, Any]]]] = []

        async def worker(r):
            async with sem:
                res = await grab_pdp(ctx, first_headers, r, args.reviews, args.reviews_limit)
                results.append(res)

        await asyncio.gather(*(worker(r) for r in all_rows))

        # ----- 3. Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ° -----
        rows_final = [row for row, _ in results]

    # ----- 4. CSV -----
    Path(OUTPUT_GOODS_CSV).write_bytes(pd.DataFrame(rows_final).to_csv(index=False).encode())
    print(f"\nğŸ‰  Ğ¢Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {len(rows_final)}  â†’  {Path(OUTPUT_GOODS_CSV).resolve()}")

    if args.reviews:
        print(f"ğŸ“  Ğ¡Ñ‹Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ (JSON-Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹) ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: {RAW_REVIEWS_DIR.resolve()}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ—Ğ°Ğ¿ÑƒÑĞº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    asyncio.run(grab())
