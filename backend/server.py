from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import asyncio
import json
import random
import time
from fake_useragent import UserAgent
from playwright.async_api import async_playwright
from playwright_stealth import Stealth
import httpx
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse, quote
import base64
import hmac, hashlib, time, base64
import socket
import requests

SECRET = "Tz178Fksyu4oAs14"

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ['MONGO_URL']
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ['DB_NAME']]

# Create the main app without a prefix
app = FastAPI(title="Chinese Tea Scraper API", version="1.0.0")

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_public_ip():
    try:
        return requests.get("https://api.ipify.org").text
    except:
        return "unknown"

logger.warning(f"⚠️ Current IP: {get_public_ip()}")


# User agent rotation
ua = UserAgent()

# Captcha solving services configuration (stubbed for now)
CAPTCHA_SERVICES = {
    "capsolver": {
        "api_key": os.environ.get("CAPSOLVER_API_KEY", "stubbed_key"),
        "client_key": os.environ.get("CAPSOLVER_CLIENT_KEY", "stubbed_key"),
        "endpoint": "https://api.capsolver.com/createTask"
    },
    "2captcha": {
        "api_key": os.environ.get("TWOCAPTCHA_API_KEY", "stubbed_key"),
        "endpoint": "https://2captcha.com/in.php"
    }
}

# Proxy configuration (stubbed)
PROXY_POOL = os.environ.get("PROXY_POOL", "").split(",") if os.environ.get("PROXY_POOL") else []

# Tea search keywords - comprehensive list
TEA_KEYWORDS = {
    "base_terms": [
        "пуэр", "пуер", "pu-erh", "pu erh", "puer",
        "китайский чай", "chinese tea", "чай китай",
        "чёрный чай", "черный чай", "black tea",
        "зелёный чай", "зеленый чай", "green tea",
        "улун", "оолонг", "oolong",
        "белый чай", "white tea",
        "дахунпао", "да хун пао", "da hong pao",
        "лунцзин", "longjing", "dragon well",
        "те гуань инь", "tie guan yin", "tieguanyin"
    ],
    "forms": [
        "блин", "блинчик", "плитка", "таблетка",
        "прессованный", "рассыпной", "листовой",
        "cake", "brick", "тuo", "то ча"
    ],
    "regions": [
        "юннань", "yunnan", "фуцзянь", "fujian",
        "аньхой", "anhui", "чжэцзян", "zhejiang",
        "гуанси", "guangxi", "гуандун", "guangdong"
    ],
    "grades": [
        "шэн", "шен", "sheng", "сырой",
        "шу", "shu", "shou", "готовый",
        "молодой", "выдержанный", "aged"
    ],
    "years": [str(year) for year in range(2010, 2025)]
}

# Pydantic models
class TeaProduct(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    ozon_id: Optional[str] = None
    sku_id: Optional[str] = None
    name: str
    price: Optional[float] = None
    original_price: Optional[float] = None
    discount: Optional[int] = None
    rating: Optional[float] = None
    reviews_count: Optional[int] = None
    description: Optional[str] = None
    characteristics: Optional[Dict[str, Any]] = None
    images: List[str] = []
    category_path: Optional[str] = None
    category_id: Optional[str] = None
    brand: Optional[str] = None
    seller: Optional[str] = None
    availability: Optional[str] = None
    delivery_info: Optional[str] = None
    weight: Optional[str] = None
    tea_type: Optional[str] = None
    tea_region: Optional[str] = None
    tea_year: Optional[str] = None
    tea_grade: Optional[str] = None
    is_pressed: Optional[bool] = None
    raw_data: Optional[Dict[str, Any]] = None
    scraped_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = None

class TeaProductCreate(BaseModel):
    name: str
    price: Optional[float] = None
    description: Optional[str] = None

class TeaReview(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    product_id: str
    ozon_review_id: Optional[str] = None
    author: Optional[str] = None
    author_id: Optional[str] = None
    rating: Optional[int] = None
    title: Optional[str] = None
    text: Optional[str] = None
    pros: Optional[str] = None
    cons: Optional[str] = None
    photos: List[str] = []
    helpful_count: Optional[int] = None
    review_date: Optional[datetime] = None
    verified_purchase: Optional[bool] = None
    seller_response: Optional[str] = None
    scraped_at: datetime = Field(default_factory=datetime.utcnow)

class ScrapingTask(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    search_term: str
    status: str = "pending"  # pending, running, completed, failed
    total_products: int = 0
    scraped_products: int = 0
    failed_products: int = 0
    error_message: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)

class ScrapingStats(BaseModel):
    total_tasks: int = 0
    running_tasks: int = 0
    completed_tasks: int = 0
    failed_tasks: int = 0
    total_products: int = 0
    captcha_solves: int = 0
    proxy_switches: int = 0
    error_rate: float = 0.0

class ProxyPool:
    def __init__(self, proxies: List[str] = None):
        self.proxies = proxies or []
        self.current_index = 0
        self.failed_proxies = set()
        
    def get_proxy(self):
        if not self.proxies:
            return None
        
        available_proxies = [p for i, p in enumerate(self.proxies) if i not in self.failed_proxies]
        if not available_proxies:
            # Reset failed proxies if all are failed
            self.failed_proxies.clear()
            available_proxies = self.proxies
        
        proxy = available_proxies[self.current_index % len(available_proxies)]
        self.current_index += 1
        return proxy
    
    def mark_failed(self, proxy: str):
        if proxy in self.proxies:
            self.failed_proxies.add(self.proxies.index(proxy))

# Global proxy pool
proxy_pool = ProxyPool(PROXY_POOL)

class CaptchaSolver:
    def __init__(self):
        self.services = CAPTCHA_SERVICES
        self.solve_count = 0
    
    async def solve_captcha(self, site_key: str, page_url: str, captcha_type: str = "hcaptcha"):
        """Solve captcha using available services (stubbed implementation)"""
        self.solve_count += 1
        
        # Stubbed implementation - in production this would make actual API calls
        logger.info(f"Solving {captcha_type} captcha for {page_url}")
        
        # Simulate solving time
        await asyncio.sleep(random.uniform(10, 30))
        
        # Return a stubbed response
        return {
            "success": True,
            "solution": "stubbed_solution_token",
            "service": "capsolver" if random.choice([True, False]) else "2captcha"
        }

captcha_solver = CaptchaSolver()

class OzonScraper:
    def __init__(self):
        self.base_url = "https://www.ozon.ru"
        self.search_url = "https://www.ozon.ru/search/"
        self.api_url = "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2"
        self.graphql_url = "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2"
        self.composer_url = "https://www.ozon.ru/api/composer-api.bx/page/json/v2"
        self.session = None
        self.browser = None
        self.page = None
        self.request_count = 0
        self.captcha_encounters = 0
        self.debug_mode = True
        self.rns_uuid = None
        self.csrf_token = None
        self.cookies = {}
        
        # Russian region settings (Moscow)
        self.region_settings = {
            "ozon_regions": "213000000",  # Moscow region code
            "geo_region": "Moscow",
            "timezone": "Europe/Moscow",
            "accept_language": "ru-RU,ru;q=0.9,en;q=0.8"
        }
        
    async def init_browser(self):
        """Initialize Playwright browser with stealth settings and Russian region"""
        self.playwright = await async_playwright().start()

        self.user_agent = ua.random
        
        # Browser configuration for stealth with Russian locale
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--disable-gpu',
                '--window-size=1920,1080',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--lang=ru-RU'
            ]
        )
        
        # Create context with Russian settings
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=self.user_agent,
            locale='ru-RU',
            timezone_id='Europe/Moscow',
            extra_http_headers={
                'Accept-Language': self.region_settings["accept_language"],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
        
        # Set Russian region cookie
        await context.add_cookies([
            {
                'name': 'ozon_regions',
                'value': self.region_settings["ozon_regions"],
                'domain': '.ozon.ru',
                'path': '/'
            }
        ])
        
        # Add stealth scripts
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ru-RU', 'ru', 'en-US', 'en'],
            });
            
            // Mock timezone
            Object.defineProperty(Intl.DateTimeFormat.prototype, 'resolvedOptions', {
                value: function() {
                    return { timeZone: 'Europe/Moscow' };
                }
            });
        """)
        
        self.page = await context.new_page()
        
        # Set proxy if available
        proxy = proxy_pool.get_proxy()
        if proxy:
            logger.info(f"Using proxy: {proxy}")
            
        # Initialize session by visiting main page and getting tokens
        await self.initialize_session()
    
    async def initialize_session(self):
        """Initialize session with proper cookies and tokens"""
        try:
            logger.info("Initializing Ozon session...")
            
            # Visit main page to get cookies and tokens
            await self.page.goto(self.base_url, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(2, 4))
            
            # Get cookies from browser
            cookies = await self.page.context.cookies()
            
            # Extract important tokens
            for cookie in cookies:
                if cookie['name'] == '__Secure-rns_uuid':
                    self.rns_uuid = cookie['value']
                    logger.info(f"Got rns_uuid: {self.rns_uuid[:20]}...")
                elif cookie['name'] == 'guest':
                    self.csrf_token = cookie['value']
                    logger.info(f"Got CSRF token: {self.csrf_token[:20]}...")
                elif cookie['name'] == 'device_uid':
                    self.device_uid = cookie['value']
                
                self.cookies[cookie['name']] = cookie['value']
            
            if not hasattr(self, "device_uid"):
                self.device_uid = str(uuid.uuid4())
            
            # Check if region selection is required
            content = await self.page.content()
            if "выберите ваш город" in content.lower() or "choose your city" in content.lower():
                logger.warning("Region selection required - attempting to set Moscow")
                await self.set_moscow_region()
                
            logger.info("Session initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing session: {e}")
    
    async def set_moscow_region(self):
        """Set Moscow as the region"""
        try:
            # Look for region selector
            region_button = await self.page.query_selector("[data-widget='regionSelector']")
            if region_button:
                await region_button.click()
                await asyncio.sleep(1)
                
                # Look for Moscow option
                moscow_option = await self.page.query_selector("text=Москва")
                if moscow_option:
                    await moscow_option.click()
                    await asyncio.sleep(2)
                    logger.info("Moscow region set successfully")
                    
        except Exception as e:
            logger.error(f"Error setting Moscow region: {e}")
    
    async def debug_log_request(self, url: str, method: str, payload: dict = None, response: dict = None):
        """Log detailed request/response for debugging"""
        if self.debug_mode:
            logger.info(f"🔍 DEBUG REQUEST: {method} {url}")
            if payload:
                logger.info(f"📤 Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
            if response:
                logger.info(f"📥 Response: {json.dumps(response, indent=2, ensure_ascii=False)[:1000]}...")
    
    async def search_products_api(self, query: str, category_id: str = None, page: int = 1) -> Dict:
        """Search products using Ozon API with proper authentication"""
        try:
            # Modern Ozon GraphQL search payload
            payload = {
                "operationName": "searchProducts",
                "variables": {
                    "query": query,
                    "page": page,
                    "categoryId": category_id or "",
                    "sort": "score",
                    "filters": [],
                    "withPromo": True,
                    "withInstallment": True,
                    "withPremium": True
                },
                "query": '''
                    query searchProducts($query: String!, $page: Int!, $categoryId: String, $sort: String, $filters: [FilterInput!], $withPromo: Boolean, $withInstallment: Boolean, $withPremium: Boolean) {
                        searchProducts(
                            query: $query
                            page: $page
                            categoryId: $categoryId
                            sort: $sort
                            filters: $filters
                            withPromo: $withPromo
                            withInstallment: $withInstallment
                            withPremium: $withPremium
                        ) {
                            products {
                                id
                                title
                                price {
                                    original
                                    current
                                }
                                rating {
                                    value
                                    count
                                }
                                images {
                                    original
                                    thumbnail
                                }
                                url
                                seller {
                                    name
                                }
                                categoryId
                                availability
                                delivery {
                                    text
                                }
                                attributes {
                                    name
                                    value
                                }
                            }
                            totalCount
                            hasNextPage
                        }
                    }
                '''
            }
            
            # Headers with authentication
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': self.region_settings["accept_language"],
                'User-Agent': ua.random,
                'Referer': f'https://www.ozon.ru/search/?text={query}',
                'Origin': 'https://www.ozon.ru',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
            }
            
            # Add authentication headers
            if self.rns_uuid:
                headers['x-o3-rns_uuid'] = self.rns_uuid
            if self.csrf_token:
                headers['x-o3-csrf-token'] = self.csrf_token
            
            # Make API request
            response = await self.page.evaluate(f'''
                async () => {{
                    const response = await fetch('{self.graphql_url}', {{
                        method: 'POST',
                        headers: {json.dumps(headers)},
                        body: JSON.stringify({json.dumps(payload)})
                    }});
                    return await response.json();
                }}
            ''')
            
            await self.debug_log_request(self.graphql_url, "POST", payload, response)
            
            return response
            
        except Exception as e:
            logger.error(f"Error in API search: {e}")
            return {}
    
    async def close_browser(self):
        """Close browser and cleanup"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def handle_anti_bot(self, page_content: str) -> bool:
        """Detect and handle anti-bot measures"""
        # Check for common anti-bot indicators
        anti_bot_indicators = [
            "cloudflare",
            "just a moment",
            "checking your browser",
            "captcha",
            "verify you are human",
            "robot"
        ]
        
        page_lower = page_content.lower()
        detected = any(indicator in page_lower for indicator in anti_bot_indicators)
        
        if detected:
            self.captcha_encounters += 1
            logger.warning(f"Anti-bot measure detected (encounter #{self.captcha_encounters})")
            
            # Try to solve captcha
            if "captcha" in page_lower:
                # Extract captcha details (simplified)
                site_key = "stubbed_site_key"  # In production, extract from page
                result = await captcha_solver.solve_captcha(site_key, self.page.url)
                
                if result["success"]:
                    logger.info(f"Captcha solved using {result['service']}")
                    # In production, submit solution and wait for redirect
                    await asyncio.sleep(5)
                    return True
            
            # Wait and retry
            await asyncio.sleep(random.uniform(30, 60))
            return False
        
        return True
    
    async def search_products(self, query: str, max_pages: int = 5) -> List[Dict]:
        """Search for tea products on Ozon with improved data extraction"""
        products = []
        
        try:
            # First try API search
            # logger.info(f"Starting API search for: {query}")
            
            # Try to get category ID for tea products
            # tea_category_id = await self.get_tea_category_id()
            
            # api_response = await self.search_products_api(query, tea_category_id)
            
            # if api_response and 'data' in api_response:
            #     search_data = api_response.get('data', {}).get('searchProducts', {})
            #     api_products = search_data.get('products', [])
                
            #     if api_products:
            #         logger.info(f"API search found {len(api_products)} products")
                    
            #         for product in api_products:
            #             try:
            #                 product_data = await self.parse_api_product(product)
            #                 if product_data:
            #                     products.append(product_data)
            #             except Exception as e:
            #                 logger.error(f"Error parsing API product: {e}")
            #                 continue
                    
            #         return products
            #     else:
            #         logger.warning("API search returned empty products list")
            
            # Try entrypoint API interception if API search fails
            logger.info("Trying entrypoint API interception scraper")
            entry_products = await self.search_products_entrypoint(query, max_pages)
            if entry_products:
                logger.info(f"Entrypoint scraper found {len(entry_products)} products")
                return entry_products

            # Fallback to HTML scraping if all else fails
            logger.info("Falling back to HTML scraping")
            return await self.search_products_html(query, max_pages)
            
        except Exception as e:
            logger.error(f"Error in search_products: {e}")
            entry_products = await self.search_products_entrypoint(query, max_pages)
            if entry_products:
                logger.info(f"Entrypoint scraper recovered {len(entry_products)} products after error")
                return entry_products
            
            # Try HTML scraping as final fallback
            return await self.search_products_html(query, max_pages)
    
    async def get_tea_category_id(self) -> str:
        """Get category ID for tea products"""
        try:
            # Navigate to tea category page
            tea_url = "https://www.ozon.ru/category/chay-9373/"
            await self.page.goto(tea_url, wait_until="domcontentloaded")
            await asyncio.sleep(2)
            
            # Extract category ID from URL or page data
            current_url = self.page.url
            category_match = re.search(r'category/.*?-(\d+)/', current_url)
            if category_match:
                category_id = category_match.group(1)
                logger.info(f"Found tea category ID: {category_id}")
                return category_id
            
            # Try to find it in page data
            content = await self.page.content()
            category_match = re.search(r'"categoryId":"(\d+)"', content)
            if category_match:
                category_id = category_match.group(1)
                logger.info(f"Extracted tea category ID from page: {category_id}")
                return category_id
            
        except Exception as e:
            logger.error(f"Error getting tea category ID: {e}")
        
        return "9373"  # Default tea category ID
    
    async def parse_api_product(self, product: Dict) -> Dict:
        """Parse product data from API response"""
        try:
            product_data = {
                "ozon_id": str(product.get("id", "")),
                "name": product.get("title", ""),
                "product_url": urljoin(self.base_url, product.get("url", "")),
                "images": []
            }
            
            # Parse price
            price_data = product.get("price", {})
            if price_data:
                product_data["price"] = price_data.get("current")
                product_data["original_price"] = price_data.get("original")
            
            # Parse rating
            rating_data = product.get("rating", {})
            if rating_data:
                product_data["rating"] = rating_data.get("value")
                product_data["reviews_count"] = rating_data.get("count")
            
            # Parse images
            images = product.get("images", [])
            if images:
                product_data["images"] = [img.get("original") or img.get("thumbnail") for img in images if img]
            
            # Parse seller
            seller_data = product.get("seller", {})
            if seller_data:
                product_data["seller"] = seller_data.get("name")
            
            # Parse availability
            product_data["availability"] = product.get("availability", "")
            
            # Parse delivery
            delivery_data = product.get("delivery", {})
            if delivery_data:
                product_data["delivery_info"] = delivery_data.get("text")
            
            # Parse attributes
            attributes = product.get("attributes", [])
            if attributes:
                product_data["characteristics"] = {
                    attr.get("name"): attr.get("value") 
                    for attr in attributes if attr.get("name")
                }
            
            # Set category
            product_data["category_id"] = product.get("categoryId")
            
            # Classify tea type
            if product_data["name"]:
                product_data.update(self.classify_tea_type(product_data["name"]))
            
            # Store raw API data for debugging
            product_data["raw_data"] = product
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error parsing API product: {e}")
            return {}
    
    async def search_products_html(self, query: str, max_pages: int = 5) -> List[Dict]:
        """Fallback HTML scraping method"""
        products = []
        
        try:
            # Navigate to search page
            search_url = f"{self.search_url}?text={query}&category=9373"
            logger.info(f"HTML search URL: {search_url}")
            
            await self.page.goto(search_url, wait_until="domcontentloaded")
            
            # Handle initial anti-bot check
            content = await self.page.content()
            if not await self.handle_anti_bot(content):
                logger.error("Failed to bypass anti-bot measures")
                return products
            
            # Wait for products to load
            await asyncio.sleep(random.uniform(3, 6))
            
            # Check for "no products found" or region selection
            if await self.check_for_no_products():
                logger.warning("No products found - possible geo-blocking")
                return products
            
            # Extract products from current page
            page_products = await self.extract_products_from_page()
            products.extend(page_products)
            
            logger.info(f"Found {len(page_products)} products on page 1")
            
            # Navigate through additional pages
            for page_num in range(2, max_pages + 1):
                if len(products) >= 50:  # Reasonable limit
                    break
                    
                try:
                    # Navigate to next page
                    next_url = f"{self.search_url}?text={query}&category=9373&page={page_num}"
                    await self.page.goto(next_url, wait_until="domcontentloaded")
                    
                    # Random delay
                    await asyncio.sleep(random.uniform(4, 8))
                    
                    # Extract products
                    page_products = await self.extract_products_from_page()
                    if not page_products:
                        break
                    
                    products.extend(page_products)
                    logger.info(f"Found {len(page_products)} products on page {page_num}")
                    
                    # Rate limiting
                    await asyncio.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    logger.error(f"Error scraping page {page_num}: {e}")
                    break
                    
        except Exception as e:
            logger.error(f"Error in HTML search: {e}")
        
        return products
    
    async def search_products_entrypoint(self, query: str, max_pages: int = 10) -> List[Dict]:
        """Scrape products using entrypoint API interception (fallback)."""
        search_url = f"{self.search_url}?text={query}&category=9373&page=1"

        async with Stealth().use_async(async_playwright()) as p:
            browser = await p.chromium.launch(headless=False)
            ctx = await browser.new_context(locale="ru-RU")
            page = await ctx.new_page()

            first_headers, first_json = {}, None

            async def save_first(resp):
                nonlocal first_headers, first_json
                if re.search(r"/api/entrypoint-api\.bx/page/json/v2\?url=%2Fsearch%2F", resp.url) and resp.status == 200 and not first_json:
                    first_headers = await resp.request.all_headers()
                    first_json = await resp.json()

            page.on("response", save_first)
            await page.goto(search_url, timeout=60000)
            await page.mouse.wheel(0, 4000)
            await asyncio.sleep(5)

            if not first_json:
                logger.warning("Entry API response not captured")
                await browser.close()
                return []

            api_base = "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2"
            all_items, json_page, page_no = [], first_json, 1

            while True:
                try:
                    grid_key = next(k for k, v in json_page["widgetStates"].items() if '"items":[' in v)
                    grid = json.loads(json_page["widgetStates"][grid_key])
                    all_items.extend(grid.get("items", []))
                except Exception as e:
                    logger.error(f"Failed to parse grid: {e}")
                    break

                if "nextUrl" in grid:
                    next_path = grid["nextUrl"]
                elif "pageInfo" in grid and "nextUrl" in grid["pageInfo"]:
                    next_path = grid["pageInfo"]["nextUrl"]
                elif "pagination" in grid and "nextUrl" in grid["pagination"]:
                    next_path = grid["pagination"]["nextUrl"]
                else:
                    break

                page_no += 1
                if max_pages and page_no > max_pages:
                    break

                next_api = f"{api_base}?url={quote(next_path, safe='')}"
                resp = await ctx.request.get(next_api, headers=first_headers)
                json_page = await resp.json()

            await browser.close()

        products = []
        for it in all_items:
            print(it)
            try:
                name = next(b for b in it.get("mainState", []) if b.get("type") == "textAtom")["textAtom"]["text"]
                price_blk = next(b for b in it.get("mainState", []) if b.get("type") == "priceV2")["priceV2"]["price"]
                price_text = next(t["text"] for t in price_blk if t.get("textStyle") == "PRICE").replace("\u2009", " ")
                url = urljoin(self.base_url, it.get("action", {}).get("link", ""))
                products.append({"name": name, "price": price_text, "product_url": url})
            except Exception:
                continue

        return products
    
    async def check_for_no_products(self) -> bool:
        """Check if page shows no products found"""
        try:
            content = await self.page.content()
            
            # Check for various "no products" indicators
            no_products_indicators = [
                "ничего не найдено",
                "товаров не найдено", 
                "нет товаров",
                "выберите ваш город",
                "choose your city",
                "nothing found",
                "no products"
            ]
            
            content_lower = content.lower()
            for indicator in no_products_indicators:
                if indicator in content_lower:
                    logger.warning(f"No products indicator found: {indicator}")
                    return True
            
            # Check for empty results container
            results_container = await self.page.query_selector("[data-widget='searchResultsV2']")
            if not results_container:
                logger.warning("No search results container found")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking for no products: {e}")
            return False
    
    async def extract_products_from_page(self) -> List[Dict]:
        """Extract product information from current page with improved selectors"""
        products = []
        
        try:
            # Wait for products to load with multiple possible selectors
            selectors_to_try = [
                "[data-widget='searchResultsV2']",
                "[data-widget='searchResults']", 
                ".search-results",
                ".product-card",
                "[data-test-id='product-card']"
            ]
            
            container = None
            for selector in selectors_to_try:
                try:
                    await self.page.wait_for_selector(selector, timeout=5000)
                    container = await self.page.query_selector(selector)
                    if container:
                        logger.info(f"Found products container with selector: {selector}")
                        break
                except:
                    continue
            
            if not container:
                logger.warning("No products container found")
                return products
            
            # Try different product card selectors
            card_selectors = [
                "[data-test-id='tile-clickable-element']",
                "[data-widget='searchResultsV2'] > div",
                ".product-card",
                "[data-test-id='product-card']",
                ".tile-clickable-element"
            ]
            
            product_cards = []
            for selector in card_selectors:
                try:
                    cards = await self.page.query_selector_all(selector)
                    if cards:
                        product_cards = cards
                        logger.info(f"Found {len(cards)} product cards with selector: {selector}")
                        break
                except:
                    continue
            
            if not product_cards:
                # Try to find products in page content
                content = await self.page.content()
                await self.save_debug_html(content)
                logger.warning("No product cards found with any selector")
                return products
            
            # Extract data from each card
            for i, card in enumerate(product_cards[:20]):  # Limit to first 20 for performance
                try:
                    product_data = await self.extract_product_data(card)
                    if product_data and product_data.get("name"):
                        products.append(product_data)
                        logger.info(f"Extracted product {i+1}: {product_data.get('name', 'Unknown')[:50]}...")
                except Exception as e:
                    logger.error(f"Error extracting product data from card {i}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error extracting products from page: {e}")
        
        return products
    
    async def save_debug_html(self, content: str):
        """Save HTML content for debugging"""
        try:
            if self.debug_mode:
                import aiofiles
                debug_file = f"/tmp/ozon_debug_{int(time.time())}.html"
                async with aiofiles.open(debug_file, 'w', encoding='utf-8') as f:
                    await f.write(content)
                logger.info(f"Debug HTML saved to: {debug_file}")
        except Exception as e:
            logger.error(f"Error saving debug HTML: {e}")
    
    async def extract_product_data(self, card_element) -> Dict:
        """Extract individual product data from card element with improved selectors"""
        product_data = {}
        
        try:
            # Try multiple name selectors
            name_selectors = [
                "[data-test-id='tile-name']",
                ".tile-name",
                "[data-test-id='product-name']",
                ".product-name",
                "h3",
                ".title"
            ]
            
            for selector in name_selectors:
                try:
                    name_element = await card_element.query_selector(selector)
                    if name_element:
                        name = await name_element.text_content()
                        if name and name.strip():
                            product_data["name"] = name.strip()
                            break
                except:
                    continue
            
            # Try multiple price selectors
            price_selectors = [
                "[data-test-id='tile-price']",
                ".tile-price", 
                "[data-test-id='product-price']",
                ".product-price",
                ".price"
            ]
            
            for selector in price_selectors:
                try:
                    price_element = await card_element.query_selector(selector)
                    if price_element:
                        price_text = await price_element.text_content()
                        if price_text:
                            # Extract numeric price
                            price_match = re.search(r'(\d+(?:\s?\d+)*)', price_text.replace(' ', ''))
                            if price_match:
                                product_data["price"] = float(price_match.group(1).replace(' ', ''))
                                break
                except:
                    continue
            
            # Try multiple rating selectors
            rating_selectors = [
                "[data-test-id='tile-rating']",
                ".tile-rating",
                "[data-test-id='product-rating']", 
                ".product-rating",
                ".rating"
            ]
            
            for selector in rating_selectors:
                try:
                    rating_element = await card_element.query_selector(selector)
                    if rating_element:
                        rating_text = await rating_element.text_content()
                        if rating_text:
                            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                            if rating_match:
                                product_data["rating"] = float(rating_match.group(1))
                                break
                except:
                    continue
            
            # Try multiple review count selectors
            review_selectors = [
                "[data-test-id='tile-review-count']",
                ".tile-review-count",
                "[data-test-id='product-reviews']",
                ".product-reviews"
            ]
            
            for selector in review_selectors:
                try:
                    reviews_element = await card_element.query_selector(selector)
                    if reviews_element:
                        reviews_text = await reviews_element.text_content()
                        if reviews_text:
                            reviews_match = re.search(r'(\d+)', reviews_text)
                            if reviews_match:
                                product_data["reviews_count"] = int(reviews_match.group(1))
                                break
                except:
                    continue
            
            # Try multiple link selectors
            link_selectors = [
                "a",
                "[data-test-id='tile-clickable']",
                ".tile-clickable"
            ]
            
            for selector in link_selectors:
                try:
                    link_element = await card_element.query_selector(selector)
                    if link_element:
                        href = await link_element.get_attribute("href")
                        if href:
                            product_data["product_url"] = urljoin(self.base_url, href)
                            # Extract product ID from URL
                            id_match = re.search(r'/product/.*?-(\d+)/', href)
                            if id_match:
                                product_data["ozon_id"] = id_match.group(1)
                            break
                except:
                    continue
            
            # Try multiple image selectors
            img_selectors = [
                "img",
                "[data-test-id='tile-image']",
                ".tile-image"
            ]
            
            for selector in img_selectors:
                try:
                    img_element = await card_element.query_selector(selector)
                    if img_element:
                        img_src = await img_element.get_attribute("src")
                        if img_src:
                            product_data["images"] = [img_src]
                            break
                except:
                    continue
            
            # Classify tea type based on name
            if product_data.get("name"):
                product_data.update(self.classify_tea_type(product_data["name"]))
            
            # Add debug info
            if self.debug_mode and product_data:
                logger.info(f"🔍 DEBUG PRODUCT: {json.dumps(product_data, ensure_ascii=False, indent=2)}")
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error extracting product data: {e}")
            return {}
    
    def classify_tea_type(self, name: str) -> Dict:
        """Classify tea type based on product name"""
        name_lower = name.lower()
        classification = {}
        
        # Detect tea type
        if any(term in name_lower for term in ["пуэр", "пуер", "pu-erh", "pu erh", "puer"]):
            classification["tea_type"] = "пуэр"
        elif any(term in name_lower for term in ["улун", "оолонг", "oolong"]):
            classification["tea_type"] = "улун"
        elif any(term in name_lower for term in ["зелёный", "зеленый", "green"]):
            classification["tea_type"] = "зелёный"
        elif any(term in name_lower for term in ["чёрный", "черный", "black"]):
            classification["tea_type"] = "чёрный"
        elif any(term in name_lower for term in ["белый", "white"]):
            classification["tea_type"] = "белый"
        
        # Detect if pressed
        if any(term in name_lower for term in ["блин", "блинчик", "плитка", "прессованный", "cake", "brick"]):
            classification["is_pressed"] = True
        elif any(term in name_lower for term in ["рассыпной", "листовой", "loose"]):
            classification["is_pressed"] = False
        
        # Detect grade for puer
        if classification.get("tea_type") == "пуэр":
            if any(term in name_lower for term in ["шэн", "шен", "sheng", "сырой"]):
                classification["tea_grade"] = "шэн"
            elif any(term in name_lower for term in ["шу", "shu", "shou", "готовый"]):
                classification["tea_grade"] = "шу"
        
        # Detect region
        for region in TEA_KEYWORDS["regions"]:
            if region in name_lower:
                classification["tea_region"] = region
                break
        
        # Detect year
        for year in TEA_KEYWORDS["years"]:
            if year in name_lower:
                classification["tea_year"] = year
                break
        
        return classification
    
    async def scrape_product_details(self, product_url: str) -> Dict:
        """Scrape detailed product information"""
        try:
            await self.page.goto(product_url, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(2, 4))
            
            # Handle anti-bot measures
            content = await self.page.content()
            if not await self.handle_anti_bot(content):
                return {}
            
            # Extract detailed information
            details = {}
            
            # Extract description
            desc_element = await self.page.query_selector("[data-widget='webProductDescription']")
            if desc_element:
                details["description"] = await desc_element.text_content()
            
            # Extract characteristics
            char_elements = await self.page.query_selector_all("[data-widget='webCharacteristics'] [data-test-id='property-row']")
            characteristics = {}
            
            for element in char_elements:
                key_element = await element.query_selector("[data-test-id='property-name']")
                value_element = await element.query_selector("[data-test-id='property-value']")
                
                if key_element and value_element:
                    key = await key_element.text_content()
                    value = await value_element.text_content()
                    characteristics[key.strip()] = value.strip()
            
            details["characteristics"] = characteristics
            
            # Extract additional images
            img_elements = await self.page.query_selector_all("[data-widget='webGallery'] img")
            images = []
            for img in img_elements:
                src = await img.get_attribute("src")
                if src:
                    images.append(src)
            details["images"] = images
            
            return details
            
        except Exception as e:
            logger.error(f"Error scraping product details: {e}")
            return {}

# Global scraper instance
scraper = OzonScraper()

# Search query generator
def generate_search_queries(limit: int = 50) -> List[str]:
    """Generate comprehensive search queries for tea products"""
    queries = []
    
    # Base queries
    queries.extend(TEA_KEYWORDS["base_terms"][:10])
    
    # Combinations
    for base_term in TEA_KEYWORDS["base_terms"][:5]:
        for form in TEA_KEYWORDS["forms"][:3]:
            queries.append(f"{base_term} {form}")
        
        for region in TEA_KEYWORDS["regions"][:3]:
            queries.append(f"{base_term} {region}")
    
    # Specific puer queries
    puer_queries = [
        "пуэр блин 357",
        "шэн пуэр",
        "шу пуэр",
        "пуэр юннань",
        "китайский пуэр",
        "пуэр чай блинчик",
        "пуэр плитка",
        "пуэр рассыпной"
    ]
    queries.extend(puer_queries)
    
    # Remove duplicates and limit
    unique_queries = list(set(queries))
    return unique_queries[:limit]

# Background task for scraping
async def scrape_tea_products_task(search_term: str, task_id: str):
    """Background task for scraping tea products with improved error handling"""
    
    # Update task status
    await db.scraping_tasks.update_one(
        {"id": task_id},
        {
            "$set": {
                "status": "running",
                "started_at": datetime.utcnow()
            }
        }
    )
    
    try:
        logger.info(f"🚀 Starting scraping task {task_id} for search term: {search_term}")
        
        # Initialize browser
        await scraper.init_browser()
        
        logger.info(f"🔍 Searching for products with term: {search_term}")
        
        # Search for products
        products = await scraper.search_products(search_term, max_pages=3)
        
        logger.info(f"📊 Found {len(products)} products to process")
        
        if not products:
            logger.warning("⚠️ No products found - this might indicate geo-blocking or API changes")
            
            # Update task with warning
            await db.scraping_tasks.update_one(
                {"id": task_id},
                {
                    "$set": {
                        "status": "completed",
                        "completed_at": datetime.utcnow(),
                        "total_products": 0,
                        "scraped_products": 0,
                        "failed_products": 0,
                        "error_message": "No products found - possible geo-blocking or API changes"
                    }
                }
            )
            return
        
        scraped_count = 0
        failed_count = 0
        
        for i, product_data in enumerate(products):
            try:
                print(product_data)

                logger.info(f"📝 Processing product {i+1}/{len(products)}: {product_data.get('name', 'Unknown')[:50]}...")
                
                # Validate product data
                if not product_data.get("name"):
                    logger.warning(f"⚠️ Product {i+1} has no name, skipping")
                    failed_count += 1
                    continue
                
                # Create tea product
                tea_product = TeaProduct(**product_data)
                
                # Check if product already exists
                existing = None
                if tea_product.ozon_id:
                    existing = await db.tea_products.find_one({"ozon_id": tea_product.ozon_id})
                
                if existing:
                    # Update existing product
                    tea_product.updated_at = datetime.utcnow()
                    await db.tea_products.update_one(
                        {"ozon_id": tea_product.ozon_id},
                        {"$set": tea_product.dict()}
                    )
                    logger.info(f"✅ Updated existing product: {tea_product.ozon_id}")
                else:
                    # Insert new product
                    await db.tea_products.insert_one(tea_product.dict())
                    logger.info(f"✅ Inserted new product: {tea_product.name[:50]}...")
                
                scraped_count += 1
                
                # Update task progress
                await db.scraping_tasks.update_one(
                    {"id": task_id},
                    {
                        "$set": {
                            "scraped_products": scraped_count,
                            "failed_products": failed_count,
                            "total_products": len(products)
                        }
                    }
                )
                
            except Exception as e:
                logger.error(f"❌ Error processing product {i+1}: {e}")
                failed_count += 1
                
                # Update task with current failure count
                await db.scraping_tasks.update_one(
                    {"id": task_id},
                    {
                        "$set": {
                            "failed_products": failed_count
                        }
                    }
                )
        
        # Update task completion
        status = "completed" if scraped_count > 0 else "failed"
        error_message = None if scraped_count > 0 else f"Failed to scrape any products out of {len(products)} found"
        
        await db.scraping_tasks.update_one(
            {"id": task_id},
            {
                "$set": {
                    "status": status,
                    "completed_at": datetime.utcnow(),
                    "total_products": len(products),
                    "scraped_products": scraped_count,
                    "failed_products": failed_count,
                    "error_message": error_message
                }
            }
        )
        
        logger.info(f"🎉 Task {task_id} completed: {scraped_count} products scraped, {failed_count} failed")
        
    except Exception as e:
        logger.error(f"❌ Critical error in scraping task {task_id}: {e}")
        
        # Update task failure
        await db.scraping_tasks.update_one(
            {"id": task_id},
            {
                "$set": {
                    "status": "failed",
                    "completed_at": datetime.utcnow(),
                    "error_message": f"Critical error: {str(e)}"
                }
            }
        )
    
    finally:
        # Close browser
        try:
            await scraper.close_browser()
            logger.info(f"🔒 Browser closed for task {task_id}")
        except Exception as e:
            logger.error(f"Error closing browser for task {task_id}: {e}")

# API Routes
@api_router.get("/")
async def root():
    return {"message": "Chinese Tea Scraper API", "version": "1.0.0"}

@api_router.post("/scrape/start")
async def start_scraping(background_tasks: BackgroundTasks, search_term: str = "пуэр"):
    """Start scraping tea products"""
    
    # Create scraping task
    task = ScrapingTask(search_term=search_term)
    await db.scraping_tasks.insert_one(task.dict())
    
    # Start background task
    background_tasks.add_task(scrape_tea_products_task, search_term, task.id)
    
    return {"task_id": task.id, "status": "started", "search_term": search_term}

@api_router.get("/scrape/status/{task_id}")
async def get_scraping_status(task_id: str):
    """Get scraping task status"""
    task = await db.scraping_tasks.find_one({"id": task_id})
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # Remove MongoDB ObjectId to avoid serialization issues
    if "_id" in task:
        del task["_id"]
    
    return task

@api_router.get("/scrape/tasks")
async def get_scraping_tasks():
    """Get all scraping tasks"""
    tasks = await db.scraping_tasks.find().sort("created_at", -1).to_list(100)
    
    # Remove MongoDB ObjectId to avoid serialization issues
    for task in tasks:
        if "_id" in task:
            del task["_id"]
    
    return tasks

@api_router.get("/products", response_model=List[TeaProduct])
async def get_tea_products(skip: int = 0, limit: int = 50, tea_type: Optional[str] = None):
    """Get tea products with optional filtering"""
    
    query = {}
    if tea_type:
        query["tea_type"] = tea_type
    
    products = await db.tea_products.find(query).skip(skip).limit(limit).sort("scraped_at", -1).to_list(limit)
    
    # Remove MongoDB ObjectId to avoid serialization issues
    for product in products:
        if "_id" in product:
            del product["_id"]
    
    return [TeaProduct(**product) for product in products]

@api_router.get("/products/{product_id}")
async def get_tea_product(product_id: str):
    """Get specific tea product"""
    product = await db.tea_products.find_one({"id": product_id})
    
    if not product:
        raise HTTPException(status_code=404, detail="Product not found")
    
    # Remove MongoDB ObjectId to avoid serialization issues
    if "_id" in product:
        del product["_id"]
    
    return TeaProduct(**product)

@api_router.delete("/products/{product_id}")
async def delete_tea_product(product_id: str):
    """Delete tea product"""
    result = await db.tea_products.delete_one({"id": product_id})
    
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Product not found")
    
    return {"message": "Product deleted successfully"}

@api_router.get("/search/queries")
async def get_search_queries(limit: int = 50):
    """Get generated search queries"""
    return {"queries": generate_search_queries(limit)}

@api_router.get("/stats")
async def get_scraping_stats():
    """Get scraping statistics"""
    
    # Count tasks by status
    total_tasks = await db.scraping_tasks.count_documents({})
    running_tasks = await db.scraping_tasks.count_documents({"status": "running"})
    completed_tasks = await db.scraping_tasks.count_documents({"status": "completed"})
    failed_tasks = await db.scraping_tasks.count_documents({"status": "failed"})
    
    # Count products
    total_products = await db.tea_products.count_documents({})
    
    # Calculate error rate
    error_rate = (failed_tasks / total_tasks * 100) if total_tasks > 0 else 0
    
    return ScrapingStats(
        total_tasks=total_tasks,
        running_tasks=running_tasks,
        completed_tasks=completed_tasks,
        failed_tasks=failed_tasks,
        total_products=total_products,
        captcha_solves=captcha_solver.solve_count,
        error_rate=error_rate
    )

@api_router.get("/categories")
async def get_tea_categories():
    """Get tea categories and types"""
    
    # Aggregate tea types from database
    pipeline = [
        {"$group": {"_id": "$tea_type", "count": {"$sum": 1}}},
        {"$sort": {"count": -1}}
    ]
    
    categories = await db.tea_products.aggregate(pipeline).to_list(100)
    
    return {
        "categories": categories,
        "total_types": len(categories)
    }

@api_router.post("/products/bulk-delete")
async def bulk_delete_products(product_ids: List[str]):
    """Bulk delete tea products"""
    result = await db.tea_products.delete_many({"id": {"$in": product_ids}})
    
    return {
        "deleted_count": result.deleted_count,
        "message": f"Deleted {result.deleted_count} products"
    }

@api_router.get("/debug/test-ozon")
async def test_ozon_connection():
    """Test endpoint to check Ozon connection and geo-blocking"""
    try:
        # Create a temporary scraper instance
        test_scraper = OzonScraper()
        await test_scraper.init_browser()
        
        # Test basic connection
        await test_scraper.page.goto("https://www.ozon.ru", wait_until="domcontentloaded")
        await asyncio.sleep(2)
        
        # Check page content
        title = await test_scraper.page.title()
        url = test_scraper.page.url
        
        # Check for geo-blocking indicators
        content = await test_scraper.page.content()
        geo_blocked = any(indicator in content.lower() for indicator in [
            "выберите ваш город", "choose your city", "select your region"
        ])
        
        # Test search page
        await test_scraper.page.goto("https://www.ozon.ru/search/?text=пуэр", wait_until="domcontentloaded")
        await asyncio.sleep(3)
        
        search_content = await test_scraper.page.content()
        no_products = "ничего не найдено" in search_content.lower()
        
        # Get cookies
        cookies = await test_scraper.page.context.cookies()
        cookie_info = {cookie['name']: cookie['value'][:20] + "..." for cookie in cookies}
        
        await test_scraper.close_browser()
        
        return {
            "status": "success",
            "title": title,
            "url": url,
            "geo_blocked": geo_blocked,
            "no_products_found": no_products,
            "cookies": cookie_info,
            "rns_uuid": test_scraper.rns_uuid[:20] + "..." if test_scraper.rns_uuid else None,
            "csrf_token": test_scraper.csrf_token[:20] + "..." if test_scraper.csrf_token else None
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@api_router.get("/debug/scraper-status")
async def get_scraper_status():
    """Get current scraper configuration and status"""
    try:
        # Get current running tasks
        running_tasks = await db.scraping_tasks.count_documents({"status": "running"})
        
        # Get scraper configuration
        scraper_config = {
            "base_url": scraper.base_url,
            "search_url": scraper.search_url,
            "debug_mode": scraper.debug_mode,
            "region_settings": scraper.region_settings,
            "request_count": scraper.request_count,
            "captcha_encounters": scraper.captcha_encounters,
            "captcha_solve_count": captcha_solver.solve_count,
            "proxy_pool_size": len(proxy_pool.proxies),
            "failed_proxies": len(proxy_pool.failed_proxies)
        }
        
        # Get recent task statistics
        recent_tasks = await db.scraping_tasks.find().sort("created_at", -1).limit(5).to_list(5)
        task_summary = []
        
        for task in recent_tasks:
            if "_id" in task:
                del task["_id"]
            task_summary.append({
                "id": task.get("id"),
                "status": task.get("status"),
                "search_term": task.get("search_term"),
                "total_products": task.get("total_products", 0),
                "error_message": task.get("error_message")
            })
        
        return {
            "status": "active",
            "running_tasks": running_tasks,
            "scraper_config": scraper_config,
            "recent_tasks": task_summary,
            "tea_keywords_count": {
                "base_terms": len(TEA_KEYWORDS["base_terms"]),
                "forms": len(TEA_KEYWORDS["forms"]),
                "regions": len(TEA_KEYWORDS["regions"]),
                "grades": len(TEA_KEYWORDS["grades"]),
                "years": len(TEA_KEYWORDS["years"])
            }
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@api_router.get("/export/csv")
async def export_products_csv():
    """Export products to CSV format"""
    products = await db.tea_products.find().to_list(10000)
    
    # Convert to CSV-like format
    csv_data = []
    for product in products:
        # Remove MongoDB ObjectId to avoid serialization issues
        if "_id" in product:
            del product["_id"]
            
        csv_data.append({
            "id": product.get("id"),
            "name": product.get("name"),
            "price": product.get("price"),
            "rating": product.get("rating"),
            "tea_type": product.get("tea_type"),
            "tea_region": product.get("tea_region"),
            "is_pressed": product.get("is_pressed"),
            "scraped_at": product.get("scraped_at")
        })
    
    return {"data": csv_data, "count": len(csv_data)}

# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()
