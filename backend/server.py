from fastapi import FastAPI, APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
from starlette.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import os
import logging
from pathlib import Path
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import asyncio
import json
import random
import time
from fake_useragent import UserAgent
from playwright.async_api import async_playwright
import httpx
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import base64

ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

# MongoDB connection
mongo_url = os.environ['MONGO_URL']
client = AsyncIOMotorClient(mongo_url)
db = client[os.environ['DB_NAME']]

# Create the main app without a prefix
app = FastAPI(title="Chinese Tea Scraper API", version="1.0.0")

# Create a router with the /api prefix
api_router = APIRouter(prefix="/api")

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# User agent rotation
ua = UserAgent()

# Captcha solving services configuration (stubbed for now)
CAPTCHA_SERVICES = {
    "capsolver": {
        "api_key": os.environ.get("CAPSOLVER_API_KEY", "stubbed_key"),
        "client_key": os.environ.get("CAPSOLVER_CLIENT_KEY", "stubbed_key"),
        "endpoint": "https://api.capsolver.com/createTask"
    },
    "2captcha": {
        "api_key": os.environ.get("TWOCAPTCHA_API_KEY", "stubbed_key"),
        "endpoint": "https://2captcha.com/in.php"
    }
}

# Proxy configuration (stubbed)
PROXY_POOL = os.environ.get("PROXY_POOL", "").split(",") if os.environ.get("PROXY_POOL") else []

# Tea search keywords - comprehensive list
TEA_KEYWORDS = {
    "base_terms": [
        "Ğ¿ÑƒÑÑ€", "Ğ¿ÑƒĞµÑ€", "pu-erh", "pu erh", "puer",
        "ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ñ‡Ğ°Ğ¹", "chinese tea", "Ñ‡Ğ°Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹",
        "Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ¹", "Ñ‡ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ¹", "black tea",
        "Ğ·ĞµĞ»Ñ‘Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ¹", "Ğ·ĞµĞ»ĞµĞ½Ñ‹Ğ¹ Ñ‡Ğ°Ğ¹", "green tea",
        "ÑƒĞ»ÑƒĞ½", "Ğ¾Ğ¾Ğ»Ğ¾Ğ½Ğ³", "oolong",
        "Ğ±ĞµĞ»Ñ‹Ğ¹ Ñ‡Ğ°Ğ¹", "white tea",
        "Ğ´Ğ°Ñ…ÑƒĞ½Ğ¿Ğ°Ğ¾", "Ğ´Ğ° Ñ…ÑƒĞ½ Ğ¿Ğ°Ğ¾", "da hong pao",
        "Ğ»ÑƒĞ½Ñ†Ğ·Ğ¸Ğ½", "longjing", "dragon well",
        "Ñ‚Ğµ Ğ³ÑƒĞ°Ğ½ÑŒ Ğ¸Ğ½ÑŒ", "tie guan yin", "tieguanyin"
    ],
    "forms": [
        "Ğ±Ğ»Ğ¸Ğ½", "Ğ±Ğ»Ğ¸Ğ½Ñ‡Ğ¸Ğº", "Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ°", "Ñ‚Ğ°Ğ±Ğ»ĞµÑ‚ĞºĞ°",
        "Ğ¿Ñ€ĞµÑÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹", "Ñ€Ğ°ÑÑÑ‹Ğ¿Ğ½Ğ¾Ğ¹", "Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹",
        "cake", "brick", "Ñ‚uo", "Ñ‚Ğ¾ Ñ‡Ğ°"
    ],
    "regions": [
        "ÑĞ½Ğ½Ğ°Ğ½ÑŒ", "yunnan", "Ñ„ÑƒÑ†Ğ·ÑĞ½ÑŒ", "fujian",
        "Ğ°Ğ½ÑŒÑ…Ğ¾Ğ¹", "anhui", "Ñ‡Ğ¶ÑÑ†Ğ·ÑĞ½", "zhejiang",
        "Ğ³ÑƒĞ°Ğ½ÑĞ¸", "guangxi", "Ğ³ÑƒĞ°Ğ½Ğ´ÑƒĞ½", "guangdong"
    ],
    "grades": [
        "ÑˆÑĞ½", "ÑˆĞµĞ½", "sheng", "ÑÑ‹Ñ€Ğ¾Ğ¹",
        "ÑˆÑƒ", "shu", "shou", "Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹",
        "Ğ¼Ğ¾Ğ»Ğ¾Ğ´Ğ¾Ğ¹", "Ğ²Ñ‹Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ½Ñ‹Ğ¹", "aged"
    ],
    "years": [str(year) for year in range(2010, 2025)]
}

# Pydantic models
class TeaProduct(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    ozon_id: Optional[str] = None
    sku_id: Optional[str] = None
    name: str
    price: Optional[float] = None
    original_price: Optional[float] = None
    discount: Optional[int] = None
    rating: Optional[float] = None
    reviews_count: Optional[int] = None
    description: Optional[str] = None
    characteristics: Optional[Dict[str, Any]] = None
    images: List[str] = []
    category_path: Optional[str] = None
    category_id: Optional[str] = None
    brand: Optional[str] = None
    seller: Optional[str] = None
    availability: Optional[str] = None
    delivery_info: Optional[str] = None
    weight: Optional[str] = None
    tea_type: Optional[str] = None
    tea_region: Optional[str] = None
    tea_year: Optional[str] = None
    tea_grade: Optional[str] = None
    is_pressed: Optional[bool] = None
    raw_data: Optional[Dict[str, Any]] = None
    scraped_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = None

class TeaProductCreate(BaseModel):
    name: str
    price: Optional[float] = None
    description: Optional[str] = None

class TeaReview(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    product_id: str
    ozon_review_id: Optional[str] = None
    author: Optional[str] = None
    author_id: Optional[str] = None
    rating: Optional[int] = None
    title: Optional[str] = None
    text: Optional[str] = None
    pros: Optional[str] = None
    cons: Optional[str] = None
    photos: List[str] = []
    helpful_count: Optional[int] = None
    review_date: Optional[datetime] = None
    verified_purchase: Optional[bool] = None
    seller_response: Optional[str] = None
    scraped_at: datetime = Field(default_factory=datetime.utcnow)

class ScrapingTask(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    search_term: str
    status: str = "pending"  # pending, running, completed, failed
    total_products: int = 0
    scraped_products: int = 0
    failed_products: int = 0
    error_message: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)

class ScrapingStats(BaseModel):
    total_tasks: int = 0
    running_tasks: int = 0
    completed_tasks: int = 0
    failed_tasks: int = 0
    total_products: int = 0
    captcha_solves: int = 0
    proxy_switches: int = 0
    error_rate: float = 0.0

class ProxyPool:
    def __init__(self, proxies: List[str] = None):
        self.proxies = proxies or []
        self.current_index = 0
        self.failed_proxies = set()
        
    def get_proxy(self):
        if not self.proxies:
            return None
        
        available_proxies = [p for i, p in enumerate(self.proxies) if i not in self.failed_proxies]
        if not available_proxies:
            # Reset failed proxies if all are failed
            self.failed_proxies.clear()
            available_proxies = self.proxies
        
        proxy = available_proxies[self.current_index % len(available_proxies)]
        self.current_index += 1
        return proxy
    
    def mark_failed(self, proxy: str):
        if proxy in self.proxies:
            self.failed_proxies.add(self.proxies.index(proxy))

# Global proxy pool
proxy_pool = ProxyPool(PROXY_POOL)

class CaptchaSolver:
    def __init__(self):
        self.services = CAPTCHA_SERVICES
        self.solve_count = 0
    
    async def solve_captcha(self, site_key: str, page_url: str, captcha_type: str = "hcaptcha"):
        """Solve captcha using available services (stubbed implementation)"""
        self.solve_count += 1
        
        # Stubbed implementation - in production this would make actual API calls
        logger.info(f"Solving {captcha_type} captcha for {page_url}")
        
        # Simulate solving time
        await asyncio.sleep(random.uniform(10, 30))
        
        # Return a stubbed response
        return {
            "success": True,
            "solution": "stubbed_solution_token",
            "service": "capsolver" if random.choice([True, False]) else "2captcha"
        }

captcha_solver = CaptchaSolver()

class OzonScraper:
    def __init__(self):
        self.base_url = "https://www.ozon.ru"
        self.search_url = "https://www.ozon.ru/search/"
        self.api_url = "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2"
        self.graphql_url = "https://www.ozon.ru/api/entrypoint-api.bx/page/json/v2"
        self.session = None
        self.browser = None
        self.page = None
        self.request_count = 0
        self.captcha_encounters = 0
        self.debug_mode = True
        self.rns_uuid = None
        self.csrf_token = None
        self.cookies = {}
        
        # Russian region settings (Moscow)
        self.region_settings = {
            "ozon_regions": "213000000",  # Moscow region code
            "geo_region": "Moscow",
            "timezone": "Europe/Moscow",
            "accept_language": "ru-RU,ru;q=0.9,en;q=0.8"
        }
        
    async def init_browser(self):
        """Initialize Playwright browser with stealth settings and Russian region"""
        self.playwright = await async_playwright().start()
        
        # Browser configuration for stealth with Russian locale
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--disable-gpu',
                '--window-size=1920,1080',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--lang=ru-RU'
            ]
        )
        
        # Create context with Russian settings
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=ua.random,
            locale='ru-RU',
            timezone_id='Europe/Moscow',
            extra_http_headers={
                'Accept-Language': self.region_settings["accept_language"],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
        
        # Set Russian region cookie
        await context.add_cookies([
            {
                'name': 'ozon_regions',
                'value': self.region_settings["ozon_regions"],
                'domain': '.ozon.ru',
                'path': '/'
            }
        ])
        
        # Add stealth scripts
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ru-RU', 'ru', 'en-US', 'en'],
            });
            
            // Mock timezone
            Object.defineProperty(Intl.DateTimeFormat.prototype, 'resolvedOptions', {
                value: function() {
                    return { timeZone: 'Europe/Moscow' };
                }
            });
        """)
        
        self.page = await context.new_page()
        
        # Set proxy if available
        proxy = proxy_pool.get_proxy()
        if proxy:
            logger.info(f"Using proxy: {proxy}")
            
        # Initialize session by visiting main page and getting tokens
        await self.initialize_session()
    
    async def initialize_session(self):
        """Initialize session with proper cookies and tokens"""
        try:
            logger.info("Initializing Ozon session...")
            
            # Visit main page to get cookies and tokens
            await self.page.goto(self.base_url, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(2, 4))
            
            # Get cookies from browser
            cookies = await self.page.context.cookies()
            
            # Extract important tokens
            for cookie in cookies:
                if cookie['name'] == '__Secure-rns_uuid':
                    self.rns_uuid = cookie['value']
                    logger.info(f"Got rns_uuid: {self.rns_uuid[:20]}...")
                elif cookie['name'] == 'guest':
                    self.csrf_token = cookie['value']
                    logger.info(f"Got CSRF token: {self.csrf_token[:20]}...")
                
                self.cookies[cookie['name']] = cookie['value']
            
            # Check if region selection is required
            content = await self.page.content()
            if "Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñˆ Ğ³Ğ¾Ñ€Ğ¾Ğ´" in content.lower() or "choose your city" in content.lower():
                logger.warning("Region selection required - attempting to set Moscow")
                await self.set_moscow_region()
                
            logger.info("Session initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing session: {e}")
    
    async def set_moscow_region(self):
        """Set Moscow as the region"""
        try:
            # Look for region selector
            region_button = await self.page.query_selector("[data-widget='regionSelector']")
            if region_button:
                await region_button.click()
                await asyncio.sleep(1)
                
                # Look for Moscow option
                moscow_option = await self.page.query_selector("text=ĞœĞ¾ÑĞºĞ²Ğ°")
                if moscow_option:
                    await moscow_option.click()
                    await asyncio.sleep(2)
                    logger.info("Moscow region set successfully")
                    
        except Exception as e:
            logger.error(f"Error setting Moscow region: {e}")
    
    async def debug_log_request(self, url: str, method: str, payload: dict = None, response: dict = None):
        """Log detailed request/response for debugging"""
        if self.debug_mode:
            logger.info(f"ğŸ” DEBUG REQUEST: {method} {url}")
            if payload:
                logger.info(f"ğŸ“¤ Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
            if response:
                logger.info(f"ğŸ“¥ Response: {json.dumps(response, indent=2, ensure_ascii=False)[:1000]}...")
    
    async def search_products_api(self, query: str, category_id: str = None, page: int = 1) -> Dict:
        """Search products using Ozon API with proper authentication"""
        try:
            # Modern Ozon GraphQL search payload
            payload = {
                "operationName": "searchProducts",
                "variables": {
                    "query": query,
                    "page": page,
                    "categoryId": category_id or "",
                    "sort": "score",
                    "filters": [],
                    "withPromo": True,
                    "withInstallment": True,
                    "withPremium": True
                },
                "query": '''
                    query searchProducts($query: String!, $page: Int!, $categoryId: String, $sort: String, $filters: [FilterInput!], $withPromo: Boolean, $withInstallment: Boolean, $withPremium: Boolean) {
                        searchProducts(
                            query: $query
                            page: $page
                            categoryId: $categoryId
                            sort: $sort
                            filters: $filters
                            withPromo: $withPromo
                            withInstallment: $withInstallment
                            withPremium: $withPremium
                        ) {
                            products {
                                id
                                title
                                price {
                                    original
                                    current
                                }
                                rating {
                                    value
                                    count
                                }
                                images {
                                    original
                                    thumbnail
                                }
                                url
                                seller {
                                    name
                                }
                                categoryId
                                availability
                                delivery {
                                    text
                                }
                                attributes {
                                    name
                                    value
                                }
                            }
                            totalCount
                            hasNextPage
                        }
                    }
                '''
            }
            
            # Headers with authentication
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': self.region_settings["accept_language"],
                'User-Agent': ua.random,
                'Referer': f'https://www.ozon.ru/search/?text={query}',
                'Origin': 'https://www.ozon.ru',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
            }
            
            # Add authentication headers
            if self.rns_uuid:
                headers['x-o3-rns_uuid'] = self.rns_uuid
            if self.csrf_token:
                headers['x-o3-csrf-token'] = self.csrf_token
            
            # Make API request
            response = await self.page.evaluate(f'''
                async () => {{
                    const response = await fetch('{self.graphql_url}', {{
                        method: 'POST',
                        headers: {json.dumps(headers)},
                        body: JSON.stringify({json.dumps(payload)})
                    }});
                    return await response.json();
                }}
            ''')
            
            await self.debug_log_request(self.graphql_url, "POST", payload, response)
            
            return response
            
        except Exception as e:
            logger.error(f"Error in API search: {e}")
            return {}
    
    async def close_browser(self):
        """Close browser and cleanup"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
    
    async def handle_anti_bot(self, page_content: str) -> bool:
        """Detect and handle anti-bot measures"""
        # Check for common anti-bot indicators
        anti_bot_indicators = [
            "cloudflare",
            "just a moment",
            "checking your browser",
            "captcha",
            "verify you are human",
            "robot"
        ]
        
        page_lower = page_content.lower()
        detected = any(indicator in page_lower for indicator in anti_bot_indicators)
        
        if detected:
            self.captcha_encounters += 1
            logger.warning(f"Anti-bot measure detected (encounter #{self.captcha_encounters})")
            
            # Try to solve captcha
            if "captcha" in page_lower:
                # Extract captcha details (simplified)
                site_key = "stubbed_site_key"  # In production, extract from page
                result = await captcha_solver.solve_captcha(site_key, self.page.url)
                
                if result["success"]:
                    logger.info(f"Captcha solved using {result['service']}")
                    # In production, submit solution and wait for redirect
                    await asyncio.sleep(5)
                    return True
            
            # Wait and retry
            await asyncio.sleep(random.uniform(30, 60))
            return False
        
        return True
    
    async def search_products(self, query: str, max_pages: int = 5) -> List[Dict]:
        """Search for tea products on Ozon with improved data extraction"""
        products = []
        
        try:
            # First try API search
            logger.info(f"Starting API search for: {query}")
            
            # Try to get category ID for tea products
            tea_category_id = await self.get_tea_category_id()
            
            api_response = await self.search_products_api(query, tea_category_id)
            
            if api_response and 'data' in api_response:
                search_data = api_response.get('data', {}).get('searchProducts', {})
                api_products = search_data.get('products', [])
                
                if api_products:
                    logger.info(f"API search found {len(api_products)} products")
                    
                    for product in api_products:
                        try:
                            product_data = await self.parse_api_product(product)
                            if product_data:
                                products.append(product_data)
                        except Exception as e:
                            logger.error(f"Error parsing API product: {e}")
                            continue
                    
                    return products
                else:
                    logger.warning("API search returned empty products list")
            
            # Fallback to HTML scraping if API fails
            logger.info("Falling back to HTML scraping")
            return await self.search_products_html(query, max_pages)
            
        except Exception as e:
            logger.error(f"Error in search_products: {e}")
            # Try HTML scraping as final fallback
            return await self.search_products_html(query, max_pages)
    
    async def get_tea_category_id(self) -> str:
        """Get category ID for tea products"""
        try:
            # Navigate to tea category page
            tea_url = "https://www.ozon.ru/category/chay-10498/"
            await self.page.goto(tea_url, wait_until="domcontentloaded")
            await asyncio.sleep(2)
            
            # Extract category ID from URL or page data
            current_url = self.page.url
            category_match = re.search(r'category/.*?-(\d+)/', current_url)
            if category_match:
                category_id = category_match.group(1)
                logger.info(f"Found tea category ID: {category_id}")
                return category_id
            
            # Try to find it in page data
            content = await self.page.content()
            category_match = re.search(r'"categoryId":"(\d+)"', content)
            if category_match:
                category_id = category_match.group(1)
                logger.info(f"Extracted tea category ID from page: {category_id}")
                return category_id
            
        except Exception as e:
            logger.error(f"Error getting tea category ID: {e}")
        
        return "10498"  # Default tea category ID
    
    async def parse_api_product(self, product: Dict) -> Dict:
        """Parse product data from API response"""
        try:
            product_data = {
                "ozon_id": str(product.get("id", "")),
                "name": product.get("title", ""),
                "product_url": urljoin(self.base_url, product.get("url", "")),
                "images": []
            }
            
            # Parse price
            price_data = product.get("price", {})
            if price_data:
                product_data["price"] = price_data.get("current")
                product_data["original_price"] = price_data.get("original")
            
            # Parse rating
            rating_data = product.get("rating", {})
            if rating_data:
                product_data["rating"] = rating_data.get("value")
                product_data["reviews_count"] = rating_data.get("count")
            
            # Parse images
            images = product.get("images", [])
            if images:
                product_data["images"] = [img.get("original") or img.get("thumbnail") for img in images if img]
            
            # Parse seller
            seller_data = product.get("seller", {})
            if seller_data:
                product_data["seller"] = seller_data.get("name")
            
            # Parse availability
            product_data["availability"] = product.get("availability", "")
            
            # Parse delivery
            delivery_data = product.get("delivery", {})
            if delivery_data:
                product_data["delivery_info"] = delivery_data.get("text")
            
            # Parse attributes
            attributes = product.get("attributes", [])
            if attributes:
                product_data["characteristics"] = {
                    attr.get("name"): attr.get("value") 
                    for attr in attributes if attr.get("name")
                }
            
            # Set category
            product_data["category_id"] = product.get("categoryId")
            
            # Classify tea type
            if product_data["name"]:
                product_data.update(self.classify_tea_type(product_data["name"]))
            
            # Store raw API data for debugging
            product_data["raw_data"] = product
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error parsing API product: {e}")
            return {}
    
    async def search_products_html(self, query: str, max_pages: int = 5) -> List[Dict]:
        """Fallback HTML scraping method"""
        products = []
        
        try:
            # Navigate to search page
            search_url = f"{self.search_url}?text={query}&category_id=10498"
            logger.info(f"HTML search URL: {search_url}")
            
            await self.page.goto(search_url, wait_until="domcontentloaded")
            
            # Handle initial anti-bot check
            content = await self.page.content()
            if not await self.handle_anti_bot(content):
                logger.error("Failed to bypass anti-bot measures")
                return products
            
            # Wait for products to load
            await asyncio.sleep(random.uniform(3, 6))
            
            # Check for "no products found" or region selection
            if await self.check_for_no_products():
                logger.warning("No products found - possible geo-blocking")
                return products
            
            # Extract products from current page
            page_products = await self.extract_products_from_page()
            products.extend(page_products)
            
            logger.info(f"Found {len(page_products)} products on page 1")
            
            # Navigate through additional pages
            for page_num in range(2, max_pages + 1):
                if len(products) >= 50:  # Reasonable limit
                    break
                    
                try:
                    # Navigate to next page
                    next_url = f"{self.search_url}?text={query}&category_id=10498&page={page_num}"
                    await self.page.goto(next_url, wait_until="domcontentloaded")
                    
                    # Random delay
                    await asyncio.sleep(random.uniform(4, 8))
                    
                    # Extract products
                    page_products = await self.extract_products_from_page()
                    if not page_products:
                        break
                    
                    products.extend(page_products)
                    logger.info(f"Found {len(page_products)} products on page {page_num}")
                    
                    # Rate limiting
                    await asyncio.sleep(random.uniform(3, 6))
                    
                except Exception as e:
                    logger.error(f"Error scraping page {page_num}: {e}")
                    break
                    
        except Exception as e:
            logger.error(f"Error in HTML search: {e}")
        
        return products
    
    async def check_for_no_products(self) -> bool:
        """Check if page shows no products found"""
        try:
            content = await self.page.content()
            
            # Check for various "no products" indicators
            no_products_indicators = [
                "Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾",
                "Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾", 
                "Ğ½ĞµÑ‚ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²",
                "Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñˆ Ğ³Ğ¾Ñ€Ğ¾Ğ´",
                "choose your city",
                "nothing found",
                "no products"
            ]
            
            content_lower = content.lower()
            for indicator in no_products_indicators:
                if indicator in content_lower:
                    logger.warning(f"No products indicator found: {indicator}")
                    return True
            
            # Check for empty results container
            results_container = await self.page.query_selector("[data-widget='searchResultsV2']")
            if not results_container:
                logger.warning("No search results container found")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking for no products: {e}")
            return False
    
    async def extract_products_from_page(self) -> List[Dict]:
        """Extract product information from current page with improved selectors"""
        products = []
        
        try:
            # Wait for products to load with multiple possible selectors
            selectors_to_try = [
                "[data-widget='searchResultsV2']",
                "[data-widget='searchResults']", 
                ".search-results",
                ".product-card",
                "[data-test-id='product-card']"
            ]
            
            container = None
            for selector in selectors_to_try:
                try:
                    await self.page.wait_for_selector(selector, timeout=5000)
                    container = await self.page.query_selector(selector)
                    if container:
                        logger.info(f"Found products container with selector: {selector}")
                        break
                except:
                    continue
            
            if not container:
                logger.warning("No products container found")
                return products
            
            # Try different product card selectors
            card_selectors = [
                "[data-test-id='tile-clickable-element']",
                "[data-widget='searchResultsV2'] > div",
                ".product-card",
                "[data-test-id='product-card']",
                ".tile-clickable-element"
            ]
            
            product_cards = []
            for selector in card_selectors:
                try:
                    cards = await self.page.query_selector_all(selector)
                    if cards:
                        product_cards = cards
                        logger.info(f"Found {len(cards)} product cards with selector: {selector}")
                        break
                except:
                    continue
            
            if not product_cards:
                # Try to find products in page content
                content = await self.page.content()
                await self.save_debug_html(content)
                logger.warning("No product cards found with any selector")
                return products
            
            # Extract data from each card
            for i, card in enumerate(product_cards[:20]):  # Limit to first 20 for performance
                try:
                    product_data = await self.extract_product_data(card)
                    if product_data and product_data.get("name"):
                        products.append(product_data)
                        logger.info(f"Extracted product {i+1}: {product_data.get('name', 'Unknown')[:50]}...")
                except Exception as e:
                    logger.error(f"Error extracting product data from card {i}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error extracting products from page: {e}")
        
        return products
    
    async def save_debug_html(self, content: str):
        """Save HTML content for debugging"""
        try:
            if self.debug_mode:
                import aiofiles
                debug_file = f"/tmp/ozon_debug_{int(time.time())}.html"
                async with aiofiles.open(debug_file, 'w', encoding='utf-8') as f:
                    await f.write(content)
                logger.info(f"Debug HTML saved to: {debug_file}")
        except Exception as e:
            logger.error(f"Error saving debug HTML: {e}")
    
    async def extract_product_data(self, card_element) -> Dict:
        """Extract individual product data from card element with improved selectors"""
        product_data = {}
        
        try:
            # Try multiple name selectors
            name_selectors = [
                "[data-test-id='tile-name']",
                ".tile-name",
                "[data-test-id='product-name']",
                ".product-name",
                "h3",
                ".title"
            ]
            
            for selector in name_selectors:
                try:
                    name_element = await card_element.query_selector(selector)
                    if name_element:
                        name = await name_element.text_content()
                        if name and name.strip():
                            product_data["name"] = name.strip()
                            break
                except:
                    continue
            
            # Try multiple price selectors
            price_selectors = [
                "[data-test-id='tile-price']",
                ".tile-price", 
                "[data-test-id='product-price']",
                ".product-price",
                ".price"
            ]
            
            for selector in price_selectors:
                try:
                    price_element = await card_element.query_selector(selector)
                    if price_element:
                        price_text = await price_element.text_content()
                        if price_text:
                            # Extract numeric price
                            price_match = re.search(r'(\d+(?:\s?\d+)*)', price_text.replace(' ', ''))
                            if price_match:
                                product_data["price"] = float(price_match.group(1).replace(' ', ''))
                                break
                except:
                    continue
            
            # Try multiple rating selectors
            rating_selectors = [
                "[data-test-id='tile-rating']",
                ".tile-rating",
                "[data-test-id='product-rating']", 
                ".product-rating",
                ".rating"
            ]
            
            for selector in rating_selectors:
                try:
                    rating_element = await card_element.query_selector(selector)
                    if rating_element:
                        rating_text = await rating_element.text_content()
                        if rating_text:
                            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                            if rating_match:
                                product_data["rating"] = float(rating_match.group(1))
                                break
                except:
                    continue
            
            # Try multiple review count selectors
            review_selectors = [
                "[data-test-id='tile-review-count']",
                ".tile-review-count",
                "[data-test-id='product-reviews']",
                ".product-reviews"
            ]
            
            for selector in review_selectors:
                try:
                    reviews_element = await card_element.query_selector(selector)
                    if reviews_element:
                        reviews_text = await reviews_element.text_content()
                        if reviews_text:
                            reviews_match = re.search(r'(\d+)', reviews_text)
                            if reviews_match:
                                product_data["reviews_count"] = int(reviews_match.group(1))
                                break
                except:
                    continue
            
            # Try multiple link selectors
            link_selectors = [
                "a",
                "[data-test-id='tile-clickable']",
                ".tile-clickable"
            ]
            
            for selector in link_selectors:
                try:
                    link_element = await card_element.query_selector(selector)
                    if link_element:
                        href = await link_element.get_attribute("href")
                        if href:
                            product_data["product_url"] = urljoin(self.base_url, href)
                            # Extract product ID from URL
                            id_match = re.search(r'/product/.*?-(\d+)/', href)
                            if id_match:
                                product_data["ozon_id"] = id_match.group(1)
                            break
                except:
                    continue
            
            # Try multiple image selectors
            img_selectors = [
                "img",
                "[data-test-id='tile-image']",
                ".tile-image"
            ]
            
            for selector in img_selectors:
                try:
                    img_element = await card_element.query_selector(selector)
                    if img_element:
                        img_src = await img_element.get_attribute("src")
                        if img_src:
                            product_data["images"] = [img_src]
                            break
                except:
                    continue
            
            # Classify tea type based on name
            if product_data.get("name"):
                product_data.update(self.classify_tea_type(product_data["name"]))
            
            # Add debug info
            if self.debug_mode and product_data:
                logger.info(f"ğŸ” DEBUG PRODUCT: {json.dumps(product_data, ensure_ascii=False, indent=2)}")
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error extracting product data: {e}")
            return {}
    
    def classify_tea_type(self, name: str) -> Dict:
        """Classify tea type based on product name"""
        name_lower = name.lower()
        classification = {}
        
        # Detect tea type
        if any(term in name_lower for term in ["Ğ¿ÑƒÑÑ€", "Ğ¿ÑƒĞµÑ€", "pu-erh", "pu erh", "puer"]):
            classification["tea_type"] = "Ğ¿ÑƒÑÑ€"
        elif any(term in name_lower for term in ["ÑƒĞ»ÑƒĞ½", "Ğ¾Ğ¾Ğ»Ğ¾Ğ½Ğ³", "oolong"]):
            classification["tea_type"] = "ÑƒĞ»ÑƒĞ½"
        elif any(term in name_lower for term in ["Ğ·ĞµĞ»Ñ‘Ğ½Ñ‹Ğ¹", "Ğ·ĞµĞ»ĞµĞ½Ñ‹Ğ¹", "green"]):
            classification["tea_type"] = "Ğ·ĞµĞ»Ñ‘Ğ½Ñ‹Ğ¹"
        elif any(term in name_lower for term in ["Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹", "Ñ‡ĞµÑ€Ğ½Ñ‹Ğ¹", "black"]):
            classification["tea_type"] = "Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹"
        elif any(term in name_lower for term in ["Ğ±ĞµĞ»Ñ‹Ğ¹", "white"]):
            classification["tea_type"] = "Ğ±ĞµĞ»Ñ‹Ğ¹"
        
        # Detect if pressed
        if any(term in name_lower for term in ["Ğ±Ğ»Ğ¸Ğ½", "Ğ±Ğ»Ğ¸Ğ½Ñ‡Ğ¸Ğº", "Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ°", "Ğ¿Ñ€ĞµÑÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹", "cake", "brick"]):
            classification["is_pressed"] = True
        elif any(term in name_lower for term in ["Ñ€Ğ°ÑÑÑ‹Ğ¿Ğ½Ğ¾Ğ¹", "Ğ»Ğ¸ÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹", "loose"]):
            classification["is_pressed"] = False
        
        # Detect grade for puer
        if classification.get("tea_type") == "Ğ¿ÑƒÑÑ€":
            if any(term in name_lower for term in ["ÑˆÑĞ½", "ÑˆĞµĞ½", "sheng", "ÑÑ‹Ñ€Ğ¾Ğ¹"]):
                classification["tea_grade"] = "ÑˆÑĞ½"
            elif any(term in name_lower for term in ["ÑˆÑƒ", "shu", "shou", "Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹"]):
                classification["tea_grade"] = "ÑˆÑƒ"
        
        # Detect region
        for region in TEA_KEYWORDS["regions"]:
            if region in name_lower:
                classification["tea_region"] = region
                break
        
        # Detect year
        for year in TEA_KEYWORDS["years"]:
            if year in name_lower:
                classification["tea_year"] = year
                break
        
        return classification
    
    async def scrape_product_details(self, product_url: str) -> Dict:
        """Scrape detailed product information"""
        try:
            await self.page.goto(product_url, wait_until="domcontentloaded")
            await asyncio.sleep(random.uniform(2, 4))
            
            # Handle anti-bot measures
            content = await self.page.content()
            if not await self.handle_anti_bot(content):
                return {}
            
            # Extract detailed information
            details = {}
            
            # Extract description
            desc_element = await self.page.query_selector("[data-widget='webProductDescription']")
            if desc_element:
                details["description"] = await desc_element.text_content()
            
            # Extract characteristics
            char_elements = await self.page.query_selector_all("[data-widget='webCharacteristics'] [data-test-id='property-row']")
            characteristics = {}
            
            for element in char_elements:
                key_element = await element.query_selector("[data-test-id='property-name']")
                value_element = await element.query_selector("[data-test-id='property-value']")
                
                if key_element and value_element:
                    key = await key_element.text_content()
                    value = await value_element.text_content()
                    characteristics[key.strip()] = value.strip()
            
            details["characteristics"] = characteristics
            
            # Extract additional images
            img_elements = await self.page.query_selector_all("[data-widget='webGallery'] img")
            images = []
            for img in img_elements:
                src = await img.get_attribute("src")
                if src:
                    images.append(src)
            details["images"] = images
            
            return details
            
        except Exception as e:
            logger.error(f"Error scraping product details: {e}")
            return {}

# Global scraper instance
scraper = OzonScraper()

# Search query generator
def generate_search_queries(limit: int = 50) -> List[str]:
    """Generate comprehensive search queries for tea products"""
    queries = []
    
    # Base queries
    queries.extend(TEA_KEYWORDS["base_terms"][:10])
    
    # Combinations
    for base_term in TEA_KEYWORDS["base_terms"][:5]:
        for form in TEA_KEYWORDS["forms"][:3]:
            queries.append(f"{base_term} {form}")
        
        for region in TEA_KEYWORDS["regions"][:3]:
            queries.append(f"{base_term} {region}")
    
    # Specific puer queries
    puer_queries = [
        "Ğ¿ÑƒÑÑ€ Ğ±Ğ»Ğ¸Ğ½ 357",
        "ÑˆÑĞ½ Ğ¿ÑƒÑÑ€",
        "ÑˆÑƒ Ğ¿ÑƒÑÑ€",
        "Ğ¿ÑƒÑÑ€ ÑĞ½Ğ½Ğ°Ğ½ÑŒ",
        "ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ¿ÑƒÑÑ€",
        "Ğ¿ÑƒÑÑ€ Ñ‡Ğ°Ğ¹ Ğ±Ğ»Ğ¸Ğ½Ñ‡Ğ¸Ğº",
        "Ğ¿ÑƒÑÑ€ Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ°",
        "Ğ¿ÑƒÑÑ€ Ñ€Ğ°ÑÑÑ‹Ğ¿Ğ½Ğ¾Ğ¹"
    ]
    queries.extend(puer_queries)
    
    # Remove duplicates and limit
    unique_queries = list(set(queries))
    return unique_queries[:limit]

# Background task for scraping
async def scrape_tea_products_task(search_term: str, task_id: str):
    """Background task for scraping tea products with improved error handling"""
    
    # Update task status
    await db.scraping_tasks.update_one(
        {"id": task_id},
        {
            "$set": {
                "status": "running",
                "started_at": datetime.utcnow()
            }
        }
    )
    
    try:
        logger.info(f"ğŸš€ Starting scraping task {task_id} for search term: {search_term}")
        
        # Initialize browser
        await scraper.init_browser()
        
        logger.info(f"ğŸ” Searching for products with term: {search_term}")
        
        # Search for products
        products = await scraper.search_products(search_term, max_pages=3)
        
        logger.info(f"ğŸ“Š Found {len(products)} products to process")
        
        if not products:
            logger.warning("âš ï¸ No products found - this might indicate geo-blocking or API changes")
            
            # Update task with warning
            await db.scraping_tasks.update_one(
                {"id": task_id},
                {
                    "$set": {
                        "status": "completed",
                        "completed_at": datetime.utcnow(),
                        "total_products": 0,
                        "scraped_products": 0,
                        "failed_products": 0,
                        "error_message": "No products found - possible geo-blocking or API changes"
                    }
                }
            )
            return
        
        scraped_count = 0
        failed_count = 0
        
        for i, product_data in enumerate(products):
            try:
                logger.info(f"ğŸ“ Processing product {i+1}/{len(products)}: {product_data.get('name', 'Unknown')[:50]}...")
                
                # Validate product data
                if not product_data.get("name"):
                    logger.warning(f"âš ï¸ Product {i+1} has no name, skipping")
                    failed_count += 1
                    continue
                
                # Create tea product
                tea_product = TeaProduct(**product_data)
                
                # Check if product already exists
                existing = None
                if tea_product.ozon_id:
                    existing = await db.tea_products.find_one({"ozon_id": tea_product.ozon_id})
                
                if existing:
                    # Update existing product
                    tea_product.updated_at = datetime.utcnow()
                    await db.tea_products.update_one(
                        {"ozon_id": tea_product.ozon_id},
                        {"$set": tea_product.dict()}
                    )
                    logger.info(f"âœ… Updated existing product: {tea_product.ozon_id}")
                else:
                    # Insert new product
                    await db.tea_products.insert_one(tea_product.dict())
                    logger.info(f"âœ… Inserted new product: {tea_product.name[:50]}...")
                
                scraped_count += 1
                
                # Update task progress
                await db.scraping_tasks.update_one(
                    {"id": task_id},
                    {
                        "$set": {
                            "scraped_products": scraped_count,
                            "failed_products": failed_count,
                            "total_products": len(products)
                        }
                    }
                )
                
            except Exception as e:
                logger.error(f"âŒ Error processing product {i+1}: {e}")
                failed_count += 1
                
                # Update task with current failure count
                await db.scraping_tasks.update_one(
                    {"id": task_id},
                    {
                        "$set": {
                            "failed_products": failed_count
                        }
                    }
                )
        
        # Update task completion
        status = "completed" if scraped_count > 0 else "failed"
        error_message = None if scraped_count > 0 else f"Failed to scrape any products out of {len(products)} found"
        
        await db.scraping_tasks.update_one(
            {"id": task_id},
            {
                "$set": {
                    "status": status,
                    "completed_at": datetime.utcnow(),
                    "total_products": len(products),
                    "scraped_products": scraped_count,
                    "failed_products": failed_count,
                    "error_message": error_message
                }
            }
        )
        
        logger.info(f"ğŸ‰ Task {task_id} completed: {scraped_count} products scraped, {failed_count} failed")
        
    except Exception as e:
        logger.error(f"âŒ Critical error in scraping task {task_id}: {e}")
        
        # Update task failure
        await db.scraping_tasks.update_one(
            {"id": task_id},
            {
                "$set": {
                    "status": "failed",
                    "completed_at": datetime.utcnow(),
                    "error_message": f"Critical error: {str(e)}"
                }
            }
        )
    
    finally:
        # Close browser
        try:
            await scraper.close_browser()
            logger.info(f"ğŸ”’ Browser closed for task {task_id}")
        except Exception as e:
            logger.error(f"Error closing browser for task {task_id}: {e}")

# API Routes
@api_router.get("/")
async def root():
    return {"message": "Chinese Tea Scraper API", "version": "1.0.0"}

@api_router.post("/scrape/start")
async def start_scraping(background_tasks: BackgroundTasks, search_term: str = "Ğ¿ÑƒÑÑ€"):
    """Start scraping tea products"""
    
    # Create scraping task
    task = ScrapingTask(search_term=search_term)
    await db.scraping_tasks.insert_one(task.dict())
    
    # Start background task
    background_tasks.add_task(scrape_tea_products_task, search_term, task.id)
    
    return {"task_id": task.id, "status": "started", "search_term": search_term}

@api_router.get("/scrape/status/{task_id}")
async def get_scraping_status(task_id: str):
    """Get scraping task status"""
    task = await db.scraping_tasks.find_one({"id": task_id})
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # Remove MongoDB ObjectId to avoid serialization issues
    if "_id" in task:
        del task["_id"]
    
    return task

@api_router.get("/scrape/tasks")
async def get_scraping_tasks():
    """Get all scraping tasks"""
    tasks = await db.scraping_tasks.find().sort("created_at", -1).to_list(100)
    
    # Remove MongoDB ObjectId to avoid serialization issues
    for task in tasks:
        if "_id" in task:
            del task["_id"]
    
    return tasks

@api_router.get("/products", response_model=List[TeaProduct])
async def get_tea_products(skip: int = 0, limit: int = 50, tea_type: Optional[str] = None):
    """Get tea products with optional filtering"""
    
    query = {}
    if tea_type:
        query["tea_type"] = tea_type
    
    products = await db.tea_products.find(query).skip(skip).limit(limit).sort("scraped_at", -1).to_list(limit)
    
    # Remove MongoDB ObjectId to avoid serialization issues
    for product in products:
        if "_id" in product:
            del product["_id"]
    
    return [TeaProduct(**product) for product in products]

@api_router.get("/products/{product_id}")
async def get_tea_product(product_id: str):
    """Get specific tea product"""
    product = await db.tea_products.find_one({"id": product_id})
    
    if not product:
        raise HTTPException(status_code=404, detail="Product not found")
    
    # Remove MongoDB ObjectId to avoid serialization issues
    if "_id" in product:
        del product["_id"]
    
    return TeaProduct(**product)

@api_router.delete("/products/{product_id}")
async def delete_tea_product(product_id: str):
    """Delete tea product"""
    result = await db.tea_products.delete_one({"id": product_id})
    
    if result.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Product not found")
    
    return {"message": "Product deleted successfully"}

@api_router.get("/search/queries")
async def get_search_queries(limit: int = 50):
    """Get generated search queries"""
    return {"queries": generate_search_queries(limit)}

@api_router.get("/stats")
async def get_scraping_stats():
    """Get scraping statistics"""
    
    # Count tasks by status
    total_tasks = await db.scraping_tasks.count_documents({})
    running_tasks = await db.scraping_tasks.count_documents({"status": "running"})
    completed_tasks = await db.scraping_tasks.count_documents({"status": "completed"})
    failed_tasks = await db.scraping_tasks.count_documents({"status": "failed"})
    
    # Count products
    total_products = await db.tea_products.count_documents({})
    
    # Calculate error rate
    error_rate = (failed_tasks / total_tasks * 100) if total_tasks > 0 else 0
    
    return ScrapingStats(
        total_tasks=total_tasks,
        running_tasks=running_tasks,
        completed_tasks=completed_tasks,
        failed_tasks=failed_tasks,
        total_products=total_products,
        captcha_solves=captcha_solver.solve_count,
        error_rate=error_rate
    )

@api_router.get("/categories")
async def get_tea_categories():
    """Get tea categories and types"""
    
    # Aggregate tea types from database
    pipeline = [
        {"$group": {"_id": "$tea_type", "count": {"$sum": 1}}},
        {"$sort": {"count": -1}}
    ]
    
    categories = await db.tea_products.aggregate(pipeline).to_list(100)
    
    return {
        "categories": categories,
        "total_types": len(categories)
    }

@api_router.post("/products/bulk-delete")
async def bulk_delete_products(product_ids: List[str]):
    """Bulk delete tea products"""
    result = await db.tea_products.delete_many({"id": {"$in": product_ids}})
    
    return {
        "deleted_count": result.deleted_count,
        "message": f"Deleted {result.deleted_count} products"
    }

@api_router.get("/debug/test-ozon")
async def test_ozon_connection():
    """Test endpoint to check Ozon connection and geo-blocking"""
    try:
        # Create a temporary scraper instance
        test_scraper = OzonScraper()
        await test_scraper.init_browser()
        
        # Test basic connection
        await test_scraper.page.goto("https://www.ozon.ru", wait_until="domcontentloaded")
        await asyncio.sleep(2)
        
        # Check page content
        title = await test_scraper.page.title()
        url = test_scraper.page.url
        
        # Check for geo-blocking indicators
        content = await test_scraper.page.content()
        geo_blocked = any(indicator in content.lower() for indicator in [
            "Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñˆ Ğ³Ğ¾Ñ€Ğ¾Ğ´", "choose your city", "select your region"
        ])
        
        # Test search page
        await test_scraper.page.goto("https://www.ozon.ru/search/?text=Ğ¿ÑƒÑÑ€", wait_until="domcontentloaded")
        await asyncio.sleep(3)
        
        search_content = await test_scraper.page.content()
        no_products = "Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾" in search_content.lower()
        
        # Get cookies
        cookies = await test_scraper.page.context.cookies()
        cookie_info = {cookie['name']: cookie['value'][:20] + "..." for cookie in cookies}
        
        await test_scraper.close_browser()
        
        return {
            "status": "success",
            "title": title,
            "url": url,
            "geo_blocked": geo_blocked,
            "no_products_found": no_products,
            "cookies": cookie_info,
            "rns_uuid": test_scraper.rns_uuid[:20] + "..." if test_scraper.rns_uuid else None,
            "csrf_token": test_scraper.csrf_token[:20] + "..." if test_scraper.csrf_token else None
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@api_router.get("/debug/scraper-status")
async def get_scraper_status():
    """Get current scraper configuration and status"""
    try:
        # Get current running tasks
        running_tasks = await db.scraping_tasks.count_documents({"status": "running"})
        
        # Get scraper configuration
        scraper_config = {
            "base_url": scraper.base_url,
            "search_url": scraper.search_url,
            "debug_mode": scraper.debug_mode,
            "region_settings": scraper.region_settings,
            "request_count": scraper.request_count,
            "captcha_encounters": scraper.captcha_encounters,
            "captcha_solve_count": captcha_solver.solve_count,
            "proxy_pool_size": len(proxy_pool.proxies),
            "failed_proxies": len(proxy_pool.failed_proxies)
        }
        
        # Get recent task statistics
        recent_tasks = await db.scraping_tasks.find().sort("created_at", -1).limit(5).to_list(5)
        task_summary = []
        
        for task in recent_tasks:
            if "_id" in task:
                del task["_id"]
            task_summary.append({
                "id": task.get("id"),
                "status": task.get("status"),
                "search_term": task.get("search_term"),
                "total_products": task.get("total_products", 0),
                "error_message": task.get("error_message")
            })
        
        return {
            "status": "active",
            "running_tasks": running_tasks,
            "scraper_config": scraper_config,
            "recent_tasks": task_summary,
            "tea_keywords_count": {
                "base_terms": len(TEA_KEYWORDS["base_terms"]),
                "forms": len(TEA_KEYWORDS["forms"]),
                "regions": len(TEA_KEYWORDS["regions"]),
                "grades": len(TEA_KEYWORDS["grades"]),
                "years": len(TEA_KEYWORDS["years"])
            }
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@api_router.get("/export/csv")
async def export_products_csv():
    """Export products to CSV format"""
    products = await db.tea_products.find().to_list(10000)
    
    # Convert to CSV-like format
    csv_data = []
    for product in products:
        # Remove MongoDB ObjectId to avoid serialization issues
        if "_id" in product:
            del product["_id"]
            
        csv_data.append({
            "id": product.get("id"),
            "name": product.get("name"),
            "price": product.get("price"),
            "rating": product.get("rating"),
            "tea_type": product.get("tea_type"),
            "tea_region": product.get("tea_region"),
            "is_pressed": product.get("is_pressed"),
            "scraped_at": product.get("scraped_at")
        })
    
    return {"data": csv_data, "count": len(csv_data)}

# Include the router in the main app
app.include_router(api_router)

app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("shutdown")
async def shutdown_db_client():
    client.close()